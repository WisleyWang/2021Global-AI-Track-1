{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "catholic-contamination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T09:17:47.264270Z",
     "start_time": "2021-05-03T09:17:42.995218Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer,AutoModelForPreTraining\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,AutoModel,BertTokenizer\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from itertools import repeat\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n",
    "from model.modeling_nezha import NeZhaPreTrainedModel,NeZhaModel,NeZhaForTokenClassification\n",
    "from model.configuration_nezha import NeZhaConfig\n",
    "import  torch.nn.functional as F\n",
    "import random\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertOutput,\n",
    "    BertPooler,\n",
    "    BertSelfOutput,\n",
    "    BertIntermediate,\n",
    "    BertOnlyMLMHead,\n",
    "    BertOnlyNSPHead,\n",
    "    BertPreTrainingHeads,\n",
    "    BERT_START_DOCSTRING,\n",
    "    BERT_INPUTS_DOCSTRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "victorian-cement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T09:17:47.464778Z",
     "start_time": "2021-05-03T09:17:47.266325Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NeZhaForSequenceClassification(NeZhaPreTrainedModel):\n",
    "    def __init__(self, config,model_name,num_labels1,num_labels2):\n",
    "        super().__init__(config)\n",
    "        self.num_labels1 = num_labels1\n",
    "        self.num_labels2=num_labels2\n",
    "        self.bert = NeZhaModel.from_pretrained(model_name)\n",
    "        self.embeddings=self.bert.embeddings\n",
    "        self.attn1=Attn(config.hidden_size)\n",
    "        self.attn2=Attn(config.hidden_size)\n",
    "\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        self.classifier1 = nn.Linear(config.hidden_size, self.num_labels1)\n",
    "        self.classifier2 = nn.Linear(config.hidden_size, self.num_labels2)\n",
    "        self.classifier3=nn.Linear(config.hidden_size,30)\n",
    "        self.predict=nn.Sigmoid()\n",
    "#         self.init_weights()\n",
    "#         if True:\n",
    "#             for p in self.bert.parameters(): # 冻结所有bert层\n",
    "#                 p.requires_grad = False\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "        pair_ids=None,\n",
    "    \n",
    "         labels1=None,\n",
    "        labels2=None,\n",
    "        labels3=None\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "    Returns:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        import torch\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "             input_ids=pair_ids\n",
    "        )        \n",
    "        att1=self.attn1(outputs[0])\n",
    "        att2=self.attn2(outputs[0])\n",
    "        pooled_output1 = self.dropout(att1)\n",
    "        pooled_output2 = self.dropout(att2)\n",
    "        logits1 = self.classifier1(pooled_output1)\n",
    "        logits2 = self.classifier2(pooled_output2)\n",
    " \n",
    "        predict1=self.predict(logits1) # 任务一\n",
    "        predict2=self.predict(logits2) # 任务二\n",
    "        ### ------------- 句子编辑距离任务\n",
    "        logits3=outputs[1]\n",
    "#         self.bert(\n",
    "#             input_ids=pair_ids\n",
    "#         )[1]\n",
    "        logits3= self.classifier3(logits3)\n",
    "        ### --------------- 类别数任务\n",
    "#         att4=self.attn4(outputs[0])\n",
    "#         logits4=self.classifier4(att4)\n",
    "#         att5=self.attn5(outputs[0])\n",
    "#         logits5=self.classifier5(att5)\n",
    "        \n",
    "        outputs = (predict1,predict2) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels1 is not None:\n",
    "            loss_fct = nn.BCELoss()\n",
    "            loss_fct2=nn.CrossEntropyLoss()\n",
    "#                 \n",
    "            loss1 = loss_fct(predict1.view(-1, self.num_labels1), labels1.view(-1, self.num_labels1))\n",
    "            loss2 = loss_fct(predict2.view(-1, self.num_labels2), labels2.view(-1, self.num_labels2))\n",
    "#             print(labels3)\n",
    "            loss3=loss_fct2(logits3.view(-1,30),labels3.view(-1))\n",
    "#             loss4=loss_fct2(logits4.view(-1,self.num_labels1),torch.sum(labels1,axis=1).long())\n",
    "#             loss5=loss_fct2(logits5.view(-1,self.num_labels2),torch.sum(labels2,axis=1).long())\n",
    "            loss=(loss1+loss2+loss3*0.5)\n",
    "            outputs = (loss,loss1+loss2) + outputs\n",
    "\n",
    "        return outputs  # (loss), predict1,predict2, (hidden_states), (attentions)\n",
    "\n",
    "class SpatialDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    空间dropout，即在指定轴方向上进行dropout，常用于Embedding层和CNN层后\n",
    "    如对于(batch, timesteps, embedding)的输入，若沿着axis=1则可对embedding的若干channel进行整体dropout\n",
    "    若沿着axis=2则可对某些token进行整体dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, drop=0.5):\n",
    "        super(SpatialDropout, self).__init__()\n",
    "        self.drop = drop\n",
    "        \n",
    "    def forward(self, inputs, noise_shape=None):\n",
    "        \"\"\"\n",
    "        @param: inputs, tensor\n",
    "        @param: noise_shape, tuple, 应当与inputs的shape一致，其中值为1的即沿着drop的轴\n",
    "        \"\"\"\n",
    "        outputs = inputs.clone()\n",
    "        if noise_shape is None:\n",
    "            noise_shape = (inputs.shape[0], *repeat(1, inputs.dim()-2), inputs.shape[-1])   # 默认沿着中间所有的shape\n",
    "        \n",
    "        self.noise_shape = noise_shape\n",
    "        if not self.training or self.drop == 0:\n",
    "            return inputs\n",
    "        else:\n",
    "            noises = self._make_noises(inputs)\n",
    "            if self.drop == 1:\n",
    "                noises.fill_(0.0)\n",
    "            else:\n",
    "                noises.bernoulli_(1 - self.drop).div_(1 - self.drop)\n",
    "            noises = noises.expand_as(inputs)    \n",
    "            outputs.mul_(noises)\n",
    "            return outputs\n",
    "            \n",
    "    def _make_noises(self, inputs):\n",
    "        return inputs.new().resize_(self.noise_shape)\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size,1)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :param src_len:\n",
    "            used for masking. NoneType or tensor in shape (B) indicating sequence length\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''   \n",
    "        att=self.attn(x)\n",
    "        att=F.tanh(att)\n",
    "        att=F.softmax(att,1)\n",
    "        att_x=att*x\n",
    "        return att_x.sum(1)\n",
    "\n",
    "from sklearn.utils import shuffle as reset\n",
    "def train_test_split(data_df, test_size=0.2, shuffle=True, random_state=None):\n",
    "    if shuffle:\n",
    "        data_df = reset(data_df, random_state=random_state)\n",
    "\n",
    "    train = data_df[int(len(data_df)*test_size):].reset_index(drop = True)\n",
    "    test  = data_df[:int(len(data_df)*test_size)].reset_index(drop = True)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "from torch.nn.functional import cross_entropy,binary_cross_entropy\n",
    "\n",
    "\n",
    "def eval(model, optimizer, validation_dataloader,output_model = './train_class/model.pth'):\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            _,_,predict1,predict2 = model(*batch)\n",
    "            predict1,predict2 = predict1.detach().cpu(),predict2.detach().cpu()\n",
    "            label_ids1,label_ids2 = batch[2].cpu(),batch[3].cpu()\n",
    "            \n",
    "            tmp_eval_accuracy = binary_cross_entropy(predict1, label_ids1.float()).item()+binary_cross_entropy(predict2, label_ids2.float()).item()\n",
    "            \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation mlogloss: {}\".format(eval_accuracy / nb_eval_steps))\n",
    "    global best_score\n",
    "    if best_score > eval_accuracy / nb_eval_steps:\n",
    "        best_score = eval_accuracy / nb_eval_steps\n",
    "        save(model, optimizer,output_model)\n",
    "        return 0\n",
    "    return 1\n",
    "def save(model, optimizer,output_model):\n",
    "    # save\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, output_model)\n",
    "    print('The best model has been saved')\n",
    "def flat_accuracy(preds, labels):\n",
    "#     print(preds,labels)\n",
    "    return -np.mean(labels*np.log(preds+1.e-7)+(1-labels)*np.log(preds+1.e-7))*10\n",
    "\n",
    "# 对抗训练\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "    def attack(self, epsilon=1, emb_name='word_emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "    def restore(self, emb_name='word_emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup.clear()\n",
    "        \n",
    "from collections import defaultdict\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "\n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "\n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "class WarmupLinearSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then linear decay.\n",
    "        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n",
    "        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n",
    "        Linearly decreases the multiplicative variable from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1, self.warmup_steps))\n",
    "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        correct_bias=correct_bias)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                step_size = group['lr']\n",
    "                if group['correct_bias']:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state['step']\n",
    "                    bias_correction2 = 1.0 - beta2 ** state['step']\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                if group['weight_decay'] > 0.0:\n",
    "                    p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n",
    "        return loss\n",
    "def build_optimizer(model, train_steps, learning_rate):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, correct_bias=False, eps=1e-8)\n",
    "    optimizer = Lookahead(optimizer, 5, 2)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=train_steps * 0.1, t_total=train_steps)\n",
    "    return optimizer, scheduler\n",
    "def to_predict(model, dataloader,output_model, with_labels=False):\n",
    "    \n",
    "    # load model\n",
    "    checkpoint = torch.load(output_model, map_location='cuda')\n",
    "#     print(checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "\n",
    "    print('-----Testing-----')\n",
    "\n",
    "    pred_label =np.zeros((len(test),29))\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            predict1,predict2 = model(*batch)\n",
    "            \n",
    "            predict1 = predict1.detach().cpu().numpy()\n",
    "#             print(predict1)\n",
    "            predict2 = predict2.detach().cpu().numpy()\n",
    "            predict=np.concatenate([predict1,predict2],axis=-1)\n",
    "            pred_label[i*batch_size:(i+1)*batch_size]=predict\n",
    "    return pred_label\n",
    "\n",
    "# M as ascent steps, alpha as ascent step size\n",
    "# X denotes input node features, y denotes labels\n",
    "def flag(model, X, y, optimizer, criterion, M, alpha):\n",
    "    \"\"\"\n",
    "    model：模型\n",
    "    X：输入节点特征矩阵\n",
    "    y：节点标签\n",
    "    optimizer：优化器\n",
    "    criterion：损失函数\n",
    "    M：每个epoch的step数\n",
    "    alpha：每个step的步长\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # pert初始化为和X形状相同、服从(-alpha, alpha)均匀分布的矩阵\n",
    "    pert = torch.FloatTensor(*X.shape).uniform_(-alpha, alpha)\n",
    "    # pert带有梯度\n",
    "    pert.requires_grad_()\n",
    "    # 为输入数据增加对抗性扰动pert\n",
    "    out = model(X+pert)\n",
    "    # 因为loss的梯度一直是累加的，所以每个step贡献1/M的grad值\n",
    "    loss = criterion(out, y)/M\n",
    "\n",
    "    # 每个epoch分为M个step，M个loss的grad进行累加，得到最终的loss\n",
    "    for _ in range(M-1):\n",
    "        loss.backward()\n",
    "        # 根据pert的grad来更新pert，alpha可以看作是pert的学习率\n",
    "        pert_data = pert.detach() + alpha*torch.sign(pert.grad.detach())\n",
    "        pert.data = pert_data.data\n",
    "        # pert梯度grad清零\n",
    "        pert.grad[:] = 0\n",
    "        # 重复对抗性扰动的训练过程\n",
    "        out = model(X+pert)\n",
    "        loss = criterion(out, y)/M\n",
    "\n",
    "    # 通过M个step累加的grad，更新model的参数\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "import contextlib    \n",
    "@contextlib.contextmanager\n",
    "def _disable_tracking_bn_stats(model):\n",
    "\n",
    "    def switch_attr(m):\n",
    "        if hasattr(m, 'track_running_stats'):\n",
    "            m.track_running_stats ^= True\n",
    "            \n",
    "    model.apply(switch_attr)\n",
    "    yield\n",
    "    model.apply(switch_attr)\n",
    "\n",
    "\n",
    "def _l2_normalize(d):\n",
    "    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
    "    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
    "    return d\n",
    "\n",
    "\n",
    "class VATLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, xi=10.0, eps=1.0, ip=1):\n",
    "        \"\"\"VAT loss\n",
    "        :param xi: hyperparameter of VAT (default: 10.0)\n",
    "        :param eps: hyperparameter of VAT (default: 1.0)\n",
    "        :param ip: iteration times of computing adv noise (default: 1)\n",
    "        \"\"\"\n",
    "        super(VATLoss, self).__init__()\n",
    "        self.xi = xi\n",
    "        self.eps = eps\n",
    "        self.ip = ip\n",
    "\n",
    "    def forward(self, model, x):\n",
    "        with torch.no_grad():\n",
    "            pred1,pred2 = model(x)\n",
    "\n",
    "        # prepare random unit tensor\n",
    "        d = torch.rand(x.shape).sub(0.5).to(x.device)\n",
    "        d = _l2_normalize(d)\n",
    "\n",
    "        with _disable_tracking_bn_stats(model):\n",
    "            # calc adversarial direction\n",
    "            for _ in range(self.ip):\n",
    "                d.requires_grad_()\n",
    "                pred_hat1,pred_hat2 = model(x + self.xi * d)\n",
    "                adv_distance1 = F.kl_div(pred_hat1, pred1, reduction='batchmean')\n",
    "                adv_distance2 = F.kl_div(pred_hat2, pred2, reduction='batchmean')\n",
    "                adv_distance=adv_distance1+adv_distance2\n",
    "                adv_distance.backward()\n",
    "                d = _l2_normalize(d.grad)\n",
    "                model.zero_grad()\n",
    "    \n",
    "            # calc LDS\n",
    "            r_adv = d * self.eps\n",
    "            pred_hat1,pred_hat2 = model(x + r_adv)\n",
    "            lds1 = F.kl_div(pred_hat1, pred1, reduction='batchmean')\n",
    "            lds2 = F.kl_div(pred_hat12, pred2, reduction='batchmean')\n",
    "            lds = lds1+lds2\n",
    "\n",
    "        return lds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "julian-performer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T09:17:47.567837Z",
     "start_time": "2021-05-03T09:17:47.466993Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "# from enum import IntEnum\n",
    "\n",
    "def stable_kl(logit, target, epsilon=1e-6, reduce=True):\n",
    "    logit = logit.view(-1, logit.size(-1)).float()\n",
    "    target = target.view(-1, target.size(-1)).float()\n",
    "    bs = logit.size(0)\n",
    "    p = F.log_softmax(logit, 1).exp()\n",
    "    y = F.log_softmax(target, 1).exp()\n",
    "    rp = -(1.0/(p + epsilon) -1 + epsilon).detach().log()\n",
    "    ry = -(1.0/(y + epsilon) -1 + epsilon).detach().log()\n",
    "    if reduce:\n",
    "        return (p* (rp- ry) * 2).sum() / bs\n",
    "    else:\n",
    "        return (p* (rp- ry) * 2).sum()\n",
    "\n",
    "\n",
    "def generate_noise(embed, mask, epsilon=1e-5):\n",
    "\t#生成与embed 同尺寸方差为epsion的符合正态分布的noise\n",
    "    noise = embed.data.new(embed.size()).normal_(0, 1) *  epsilon\n",
    "    noise.detach()\n",
    "    noise.requires_grad_()\n",
    "    return noise\n",
    "\n",
    "class SmartPerturbation():\n",
    "    def __init__(self,\n",
    "                 epsilon=1e-6,\n",
    "                 multi_gpu_on=False,\n",
    "                 step_size=0.1,\n",
    "                 noise_var=1e-5,\n",
    "                 norm_p='inf',\n",
    "                 k=1, # 扰动次数\n",
    "                 fp16=False,\n",
    "                 encoder_type=1,#EncoderModelType.BERT, # 4\n",
    "                 loss_map=nn.BCELoss(), # 这个是用来与taskid 对应的，不同的任务对应不同的loss，我们可以直接固定住\n",
    "                 norm_level=0):\n",
    "        super(SmartPerturbation, self).__init__()\n",
    "        self.epsilon = epsilon \n",
    "        # eta 更新扰动后的x_i的学习率\n",
    "        self.step_size = step_size\n",
    "        self.multi_gpu_on = multi_gpu_on\n",
    "        self.fp16 = fp16\n",
    "        self.K = k\n",
    "        # sigma 生成扰动噪音的方差\n",
    "        self.noise_var = noise_var \n",
    "        self.norm_p = norm_p\n",
    "        self.encoder_type = encoder_type \n",
    "        self.loss_map = loss_map \n",
    "        self.norm_level = norm_level > 0\n",
    "#         assert len(loss_map) > 0\n",
    "\n",
    "\n",
    "    def _norm_grad(self, grad, eff_grad=None, sentence_level=False):\n",
    "    \t# 计算梯度 以及 有效梯度的 方向\n",
    "        if self.norm_p == 'l2':\n",
    "            if sentence_level:\n",
    "                direction = grad / (torch.norm(grad, dim=(-2, -1), keepdim=True) + self.epsilon)\n",
    "            else:\n",
    "                direction = grad / (torch.norm(grad, dim=-1, keepdim=True) + self.epsilon)\n",
    "        elif self.norm_p == 'l1':\n",
    "            direction = grad.sign()\n",
    "        else:\n",
    "            if sentence_level:\n",
    "                direction = grad / (grad.abs().max((-2, -1), keepdim=True)[0] + self.epsilon)\n",
    "            else:\n",
    "                direction = grad / (grad.abs().max(-1, keepdim=True)[0] + self.epsilon)\n",
    "                eff_direction = eff_grad / (grad.abs().max(-1, keepdim=True)[0] + self.epsilon)\n",
    "        return direction, eff_direction\n",
    "\n",
    "    def forward(self, model,\n",
    "                logits1, # 因为我是双任务\n",
    "                logits2,\n",
    "                inputs, # 数据的输入\n",
    "                task_id=0,\n",
    "                task_type=1,#TaskType.Classification,\n",
    "                pairwise=1):\n",
    "        # adv training\n",
    "        assert task_type in set([1,2,3,4]), 'Donot support {} yet'.format(task_type)\n",
    "        \n",
    "\n",
    "        # init delta\n",
    "        # 输出 embded \n",
    "        embed = model.embeddings(inputs['input_ids']) #得到输入矩阵\n",
    "        noise = generate_noise(embed, inputs['attention_mask'], epsilon=self.noise_var)\n",
    "        vat_args = inputs\n",
    "        vat_args.pop('input_ids',1)\n",
    "        for step in range(0, self.K):\n",
    "            vat_args.update({'inputs_embeds':embed + noise})\n",
    "            \n",
    "            # 使用加入噪音的embed 输出预测结果\n",
    "            _,adv_logits1,adv_logits2 = model(**vat_args) # 双任务\n",
    "            if task_type == 2: # 回归任务\n",
    "            \t# 回归问题使用 mse loss 评估与原始embedded输出的差异\n",
    "                adv_loss = F.mse_loss(adv_logits, logits.detach(), reduction='sum')\n",
    "            else:\n",
    "                if task_type == 3:  # 排序任务\n",
    "                    adv_logits = adv_logits.view(-1, pairwise)\n",
    "                # 排序或者分类使用kl散度衡量两者之间的差异  （其他任务）\n",
    "                adv_loss = stable_kl(adv_logits1, logits1.detach(), reduce=False)+stable_kl(adv_logits2, logits2.detach(), reduce=False)\n",
    "            #  分布损失与 扰动之间的梯度\n",
    "            delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
    "            # 梯度的范数\n",
    "            norm = delta_grad.norm()\n",
    "            if (torch.isnan(norm) or torch.isinf(norm)):\n",
    "                return 0\n",
    "            # 更新到主要训练过程中的梯度 为扰动与原始输出差异损失对扰动求出的梯度 乘以 扰动的学习率\n",
    "            eff_delta_grad = delta_grad * self.step_size\n",
    "            # \n",
    "            delta_grad = noise + delta_grad * self.step_size\n",
    "            noise, eff_noise = self._norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=self.norm_level)\n",
    "            noise = noise.detach()\n",
    "            noise.requires_grad_()\n",
    "        vat_args.update({'inputs_embeds':embed + noise})\n",
    "        \n",
    "#         vat_args.pop('input_ids',default=1)\n",
    "#         adv_loss,_,_ = model(**vat_args)\n",
    "        _,adv_logits1,adv_logits2 = model(**vat_args) # 双任务\n",
    "        if task_type == 3: # 排序任务\n",
    "            adv_logits = adv_logits.view(-1, pairwise)\n",
    "#         adv_lc = self.loss_map\n",
    "#         adv_loss = adv_lc(logits, adv_logits, ignore_index=-1) #其实这里就是算个Loss,我的模型会自动返回loss\n",
    "        adv_loss=stable_kl(adv_logits1, logits1.detach(), reduce=False)+stable_kl(adv_logits2, logits2.detach(), reduce=False)\n",
    "        return adv_loss, embed.detach().abs().mean(), eff_noise.detach().abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aerial-delay",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T09:17:47.666886Z",
     "start_time": "2021-05-03T09:17:47.569954Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "def read_data(df,tokenizer,maxlen,with_labels):\n",
    "    outputs = defaultdict(list)\n",
    "    def get_label(x,num):\n",
    "        label=[0]*num\n",
    "       \n",
    "        x=x.strip().split(' ')\n",
    "\n",
    "        for l in x:              \n",
    "            if l and l!='nan':\n",
    "                label[int(l)]=1\n",
    "        return label\n",
    "    for index in tqdm(range(len(df))):\n",
    "        sent=df.loc[index, 'sentence']\n",
    "        pair_idx=random.randint(0,len(df)-1)\n",
    "        pair_sent=str(df.loc[pair_idx,'sentence'])\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        outputs['input_ids'].append(tokenizer.encode_plus(sent,\n",
    "#                                       padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,       # Truncate to max_length\n",
    "                                      max_length=maxlen,return_token_type_ids=False,  \n",
    "                                      return_tensors='pt')['input_ids'].squeeze(0)) # Return torch.Tensor objects\n",
    "        \n",
    "        outputs['pair_ids'].append(tokenizer.encode_plus(sent,pair_sent,\n",
    "#                                       padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,max_length=maxlen,  \n",
    "                                      return_tensors='pt')['input_ids'].squeeze(0))\n",
    "  \n",
    "        if with_labels:  # True if the dataset has labels\n",
    "            lab1=get_label(str(df.loc[index, 'label1']),17)\n",
    "            lab2=get_label(str(df.loc[index, 'label2']),12)\n",
    "            pair_lab1=get_label(str(df.loc[pair_idx, 'label1']),17)\n",
    "            pair_lab2=get_label(str(df.loc[pair_idx, 'label2']),12)\n",
    "            #计算编辑距离\n",
    "            outputs['labels3'].append(torch.Tensor([sum([ lab1[i]!=pair_lab1[i] for i in range(len(lab1))])+ sum([ lab2[i]!=pair_lab2[i] for i in range(len(lab2))])]).long())\n",
    "            outputs['labels1'].append(torch.Tensor(lab1))\n",
    "            outputs['labels2'].append(torch.Tensor(lab2))\n",
    "            \n",
    "    return outputs\n",
    "    \n",
    "class CustomDataset(Data.Dataset):\n",
    "    def __init__(self, data,with_labels=True):\n",
    "        self.data = data  # pandas dataframe\n",
    "\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = tokenizer#AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "        self.with_labels = with_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        outputs=self.data['input_ids'][index],self.data['pair_ids'][index]\n",
    "        if self.with_labels:\n",
    "            return self.data['input_ids'][index],self.data['pair_ids'][index],self.data['labels1'][index],self.data['labels2'][index],self.data['labels3'][index]\n",
    "           \n",
    "        return self.data['input_ids'][index],self.data['pair_ids'][index]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smooth-order",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T09:17:47.750332Z",
     "start_time": "2021-05-03T09:17:47.668648Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Data.Dataset):\n",
    "    def __init__(self, data,tokenizer,maxlen,with_labels=True):\n",
    "        self.data = data  # pandas dataframe\n",
    "\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = tokenizer#AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "        self.maxlen=maxlen\n",
    "        self.with_labels = with_labels\n",
    "    def get_label(self,x,num):\n",
    "        label=[0]*num\n",
    "       \n",
    "        x=x.strip().split(' ')\n",
    "\n",
    "        for l in x:              \n",
    "            if l and l!='nan':\n",
    "                label[int(l)]=1\n",
    "        return label\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        outputs=[]\n",
    "        sent=self.data.loc[index, 'sentence']\n",
    "        pair_idx=random.randint(0,len(self.data)-1)\n",
    "        pair_sent=str(self.data.loc[pair_idx,'sentence'])\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        outputs.append(self.tokenizer.encode_plus(sent,\n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,       # Truncate to max_length\n",
    "                                      max_length=100,return_token_type_ids=False,\n",
    "                                                 return_tensors='pt')['input_ids'].squeeze(0)) # Return torch.Tensor objects\n",
    "        \n",
    "        outputs.append(self.tokenizer.encode_plus(sent,pair_sent,\n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,max_length=self.maxlen, \n",
    "                                     return_tensors='pt')['input_ids'].squeeze(0))  \n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            lab1=self.get_label(str(self.data.loc[index, 'label1']),17)\n",
    "            lab2=self.get_label(str(self.data.loc[index, 'label2']),12)\n",
    "            pair_lab1=self.get_label(str(self.data.loc[pair_idx, 'label1']),17)\n",
    "            pair_lab2=self.get_label(str(self.data.loc[pair_idx, 'label2']),12)\n",
    "            #计算编辑距离          \n",
    "            outputs.append(torch.Tensor(lab1))\n",
    "            outputs.append(torch.Tensor(lab2))\n",
    "            outputs.append(torch.Tensor([sum([ lab1[i]!=pair_lab1[i] for i in range(len(lab1))])+ sum([ lab2[i]!=pair_lab2[i] for i in range(len(lab2))])]).long())\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "touched-debut",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T09:17:47.914090Z",
     "start_time": "2021-05-03T09:17:47.752107Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../tcdata/train.csv',header=None)\n",
    "# test=train.iloc[-2000:].copy().reset_index(drop=True)\n",
    "# train=train.iloc[:-2000]\n",
    "test=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "# test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "model_path='../model_weight/nezha/'\n",
    "output_model='../tmp/nezha.pth'\n",
    "batch_size=32\n",
    "# 合并训练集与测试集 制作特征\n",
    "for i in range(1,3):\n",
    "    train[i]=train[i].apply(lambda x:x.replace('|','').strip())\n",
    "for i in range(1,2):\n",
    "    test[i]=test[i].apply(lambda x:x.replace('|','').strip())\n",
    "train.columns=['idx','sentence','label1','label2']\n",
    "test.columns=['idx','sentence']\n",
    "# test.columns=['idx','sentence','label1','label2']\n",
    "tokenizer=BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "config=NeZhaConfig.from_pretrained(model_path,num_labels=17,hidden_dropout_prob=0.1) # config.output_attentions=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surface-billion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T09:17:47.923738Z",
     "start_time": "2021-05-03T09:17:47.915940Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_set,val_set,test_oof):\n",
    "    \n",
    "        ###--------------------\n",
    "    early_stop=0\n",
    "    print(\"Reading training data...\")\n",
    "    train_loader = Data.DataLoader(train_set, batch_size=batch_size, num_workers=8, shuffle=True,pin_memory=True)\n",
    "    print(\"Reading validation data...\")\n",
    "    val_loader = Data.DataLoader(val_set, batch_size=batch_size, num_workers=8, shuffle=True,pin_memory=True)\n",
    "    # 准备模型\n",
    "    model=NeZhaForSequenceClassification(config=config,model_name=model_path,num_labels1=17,num_labels2=12)\n",
    "    ### 训练\n",
    "    model.to(device)\n",
    "    fgm = FGM(model)\n",
    "    adv_teacher=SmartPerturbation()\n",
    "# vat\n",
    "#     vat_loss = VATLoss(xi=10.0, eps=1.0, ip=1)\n",
    "    \n",
    "    train_num = len(train_set)\n",
    "    train_steps = int(train_num * epochs / batch_size) + 1\n",
    "\n",
    "    optimizer, scheduler = build_optimizer(model, train_steps, learning_rate=6e-5)\n",
    "    print('-----Training-----')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        print('Epoch', epoch)\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            loss,r_loss,predict1,predict2 = model(*batch)    \n",
    "            if i % 50 == 0:\n",
    "                print(i, r_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # 对抗训练\n",
    "#             fgm.attack()\n",
    "#             loss_adv,_, _,_  = model(*batch)\n",
    "#             loss_adv.backward()\n",
    "#             fgm.restore()\n",
    "#----------------------------\n",
    "#             smart\n",
    "            # adv training\n",
    "            \n",
    "#             adv_inputs={'model':model,'logits1':predict1,'logits2':predict2,'inputs':inputs}\n",
    "#             adv_loss, emb_val, eff_perturb = adv_teacher.forward(**adv_inputs)\n",
    "#             loss = loss + 0.1* adv_loss\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        if epoch>-1:\n",
    "            early_stop+=eval(model, optimizer, val_loader, output_model=output_model)\n",
    "        if early_stop==2:\n",
    "            break\n",
    "\n",
    "    test_oof.append(to_predict(model, test_loader,output_model, with_labels=False))\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return test_oof   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "received-organ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T09:26:47.207915Z",
     "start_time": "2021-05-03T09:17:47.925590Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----kfold:0 --------\n",
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training-----\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-2-03dc72c881a6>:361: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370117127/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4648200273513794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/547 [00:14<02:20,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.3786814212799072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [00:31<04:01,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.32473745942115784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [01:12<07:41,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.3613997995853424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [02:01<05:41,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.3402408957481384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 250/547 [02:48<04:13,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.2397443950176239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [03:37<03:53,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.17236624658107758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 350/547 [04:24<04:12,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 0.2792940139770508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [05:15<02:26,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.21835115551948547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 450/547 [06:02<01:04,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.2310740202665329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [06:48<00:31,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.20069976150989532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [07:30<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.27589923572502556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/547 [00:00<07:53,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2549590468406677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 48/547 [00:37<06:32,  1.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a02ef8fbfc3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f7a9ebd438c5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_set, val_set, test_oof)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;31m# 对抗训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#             fgm.attack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_fold=KFold(8,shuffle=True,random_state=1080)\n",
    "test_oof=0\n",
    "epochs = 12\n",
    "test_oof=[]\n",
    "ixdfold=0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_set = CustomDataset(test,tokenizer=tokenizer,maxlen=128,with_labels=False)\n",
    "test_loader = Data.DataLoader(test_set, batch_size=batch_size, num_workers=8, shuffle=False,pin_memory=True)\n",
    "for trn_idx,val_idx in n_fold.split(train):\n",
    "    print('----kfold:%d --------'%ixdfold)\n",
    "    ixdfold+=1\n",
    "    train_df=train.iloc[trn_idx].reset_index(drop=True)\n",
    "    val_df=train.iloc[val_idx].reset_index(drop=True)\n",
    "    train_data=CustomDataset(train_df,tokenizer=tokenizer,maxlen=100,with_labels=True)\n",
    "    val_data=CustomDataset(val_df,tokenizer=tokenizer,maxlen=100,with_labels=True)\n",
    "    best_score = float('inf')\n",
    "    train_model(train_data,val_data,test_oof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  原始，分开，FGM，word_emb，多任务,xlnet,多任务改回归 ，更多任务,多任务权重微调,nezha_new\n",
    "# epoch0=0.1363  -> 0.1165->0.087-> 0.0855    ->0.095 ->0.070  ->0.135 ->0.183- >0.066 ->0.073\n",
    "# epoch1=0.160 -> 0.0893          -> 0.072    ->0.0678-> 0.0473 ->0.0877->0.133     ->0.0476\n",
    "# epoch2=0.110 -> 0.059                       ->0.0596                               ->0.038\n",
    "#                                               -> 0.0523\n",
    "#                                             ->0.041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adopted-struggle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T07:00:29.947549Z",
     "start_time": "2021-04-17T07:00:29.773942Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-95ede7728bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'report_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%.12f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_oof\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# sub.to_csv('../result.csv',index=False,header=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "sub=pd.DataFrame()\n",
    "test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "sub['report_ID']=test[0]\n",
    "sub['Prediction']=[ '|'+' '.join(['%.12f'%j for j in i]) for i in test_oof ]\n",
    "sub.to_csv('../result.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-captain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
