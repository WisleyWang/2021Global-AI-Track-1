{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fantastic-latex",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T13:32:27.082858Z",
     "start_time": "2021-05-10T13:32:27.015272Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer,AutoModelForPreTraining\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,AutoModel,BertTokenizer\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n",
    "from model.modeling_nezha import NeZhaForSequenceClassification,NeZhaPreTrainedModel,NeZhaModel,NeZhaForTokenClassification\n",
    "from model.configuration_nezha import NeZhaConfig\n",
    "import  torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertOutput,\n",
    "    BertPooler,\n",
    "    BertSelfOutput,\n",
    "    BertIntermediate,\n",
    "    BertOnlyMLMHead,\n",
    "    BertOnlyNSPHead,\n",
    "    BertPreTrainingHeads,\n",
    "    BERT_START_DOCSTRING,\n",
    "    BERT_INPUTS_DOCSTRING,\n",
    ")\n",
    "from transformers import XLNetModel,XLNetConfig\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "class NeZhaForSequenceClassification(NeZhaPreTrainedModel):\n",
    "    def __init__(self, config,model_name,num_labels1,num_labels2):\n",
    "        super().__init__(config)\n",
    "        self.num_labels1 = num_labels1\n",
    "        self.num_labels2=num_labels2\n",
    "        self.bert = XLNetModel(config).from_pretrained(model_name)\n",
    "        self.attn1=Attn(config.hidden_size)\n",
    "        self.attn2=Attn(config.hidden_size)\n",
    "        self.attn3=Attn(config.hidden_size)\n",
    "        self.attn4=Attn(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dropouts=nn.ModuleList([nn.Dropout(p) for p in np.linspace(0.1,0.5,3)])\n",
    "        self.classifier1 = nn.Linear(config.hidden_size, self.num_labels1-8)\n",
    "        self.classifier3 = nn.Linear(config.hidden_size, 8)\n",
    "        self.classifier2 = nn.Linear(config.hidden_size, self.num_labels2-6)\n",
    "        self.classifier4 = nn.Linear(config.hidden_size,6)\n",
    "        \n",
    "        self.predict=nn.Sigmoid()\n",
    "#         self.init_weights()\n",
    "#         if True:\n",
    "#             for p in self.bert.parameters(): # 冻结所有bert层\n",
    "#                 p.requires_grad = False\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            labels1=None,\n",
    "        labels2=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "    Returns:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        import torch\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        att1=self.attn1(outputs[0])\n",
    "        att2=self.attn2(outputs[0])\n",
    "        att3=self.attn3(outputs[0])\n",
    "        att4=self.attn4(outputs[0])\n",
    "        pooled_output1 = torch.stack([ dd(att1)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        pooled_output2 = torch.stack([ dd(att2)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        pooled_output3 = torch.stack([ dd(att3)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        pooled_output4 = torch.stack([ dd(att4)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        logits1 = self.classifier1(pooled_output1)\n",
    "        logits2 = self.classifier2(pooled_output2)\n",
    "        logits3 = self.classifier3(pooled_output3)\n",
    "        logits4 = self.classifier4(pooled_output4)\n",
    "        \n",
    "        logits1=torch.cat([logits1,logits3],dim=-1)\n",
    "        logits2=torch.cat([logits2,logits4],dim=-1)\n",
    "        predict1=self.predict(logits1)\n",
    "        predict2=self.predict(logits2)\n",
    "        outputs = (predict1,predict2) + outputs[2:]  # add hidden states and attention if they are here\n",
    "#         print('label:',labels)\n",
    "#         print('input_ids:',input_ids)\n",
    "#         print('attention_mas:',attention_mask)\n",
    "        if labels1 is not None:\n",
    "            loss_fct = nn.BCELoss()\n",
    "#                 print(logits.view(-1, self.num_labels))\n",
    "#                 print(labels.view(-1, self.num_labels))\n",
    "            loss1 = loss_fct(predict1.view(-1, self.num_labels1), labels1.view(-1, self.num_labels1))\n",
    "            loss2 = loss_fct(predict2.view(-1, self.num_labels2), labels2.view(-1, self.num_labels2))\n",
    "            loss=loss1+loss2\n",
    "            outputs = (loss,) + outputs\n",
    "#         print(outputs)\n",
    "        return outputs  # (loss), predict1,predict2, (hidden_states), (attentions)\n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size,1)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :param src_len:\n",
    "            used for masking. NoneType or tensor in shape (B) indicating sequence length\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''   \n",
    "        att=self.attn(x)\n",
    "        att=F.tanh(att)\n",
    "        att=F.softmax(att,1)\n",
    "        att_x=att*x\n",
    "        return att_x.sum(1)\n",
    "import torch.utils.data as Data\n",
    "class CustomDataset(Data.Dataset):\n",
    "    def __init__(self, data, maxlen,tokenizer,with_labels=True, model_name='bert-base-chinese'):\n",
    "        self.data = data  # pandas dataframe\n",
    "\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = tokenizer#AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def get_label(self,x,num):\n",
    "        label=[0]*num\n",
    "       \n",
    "        x=x.strip().split(' ')\n",
    "\n",
    "        for l in x:              \n",
    "            if l and l!='nan':\n",
    "                label[int(l)]=1\n",
    "        return label\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        sent = str(self.data.loc[index, 'sentence'])\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent,\n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,       # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
    "#         print(encoded_pair['input_ids'])\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "#         token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            label1 = torch.Tensor(self.get_label(str(self.data.loc[index, 'label1']),17))\n",
    "            label2 = torch.Tensor(self.get_label(str(self.data.loc[index, 'label2']),12))\n",
    "            return token_ids, attn_masks,label1,label2\n",
    "        else:\n",
    "            return token_ids, attn_masks\n",
    "from sklearn.utils import shuffle as reset\n",
    "def train_test_split(data_df, test_size=0.2, shuffle=True, random_state=None):\n",
    "    if shuffle:\n",
    "        data_df = reset(data_df, random_state=random_state)\n",
    "\n",
    "    train = data_df[int(len(data_df)*test_size):].reset_index(drop = True)\n",
    "    test  = data_df[:int(len(data_df)*test_size)].reset_index(drop = True)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "from torch.nn.functional import cross_entropy,binary_cross_entropy\n",
    "\n",
    "\n",
    "def evals(model, optimizer, validation_dataloader,output_model = './train_class/model.pth'):\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            predict1,predict2 = model(batch[0], batch[1])\n",
    "            predict1,predict2 = predict1.detach().cpu(),predict2.detach().cpu()\n",
    "            label_ids1,label_ids2 = batch[2].cpu(),batch[3].cpu()\n",
    "            \n",
    "            tmp_eval_accuracy = binary_cross_entropy(predict1, label_ids1.float()).item()+binary_cross_entropy(predict2, label_ids2.float()).item()\n",
    "            \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation mlogloss: {}\".format(eval_accuracy / nb_eval_steps))\n",
    "    global best_score\n",
    "    if best_score > eval_accuracy / nb_eval_steps:\n",
    "        best_score = eval_accuracy / nb_eval_steps\n",
    "        save(model, optimizer,output_model)\n",
    "        return 0\n",
    "    return 1\n",
    "def save(model, optimizer,output_model):\n",
    "    # save\n",
    "    torch.save(model, output_model)\n",
    "    print('The best model has been saved')\n",
    "def flat_accuracy(preds, labels):\n",
    "#     print(preds,labels)\n",
    "    return -np.mean(labels*np.log(preds+1.e-7)+(1-labels)*np.log(preds+1.e-7))*10\n",
    "\n",
    "# 对抗训练\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "    def attack(self, epsilon=1.0, emb_name='word_emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "    def restore(self, emb_name='word_emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        \n",
    "from collections import defaultdict\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "\n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "\n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "class WarmupLinearSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then linear decay.\n",
    "        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n",
    "        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n",
    "        Linearly decreases the multiplicative variable from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1, self.warmup_steps))\n",
    "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        correct_bias=correct_bias)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                step_size = group['lr']\n",
    "                if group['correct_bias']:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state['step']\n",
    "                    bias_correction2 = 1.0 - beta2 ** state['step']\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                if group['weight_decay'] > 0.0:\n",
    "                    p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n",
    "        return loss\n",
    "def build_optimizer(model, train_steps, learning_rate):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, correct_bias=False, eps=1e-8)\n",
    "    optimizer = Lookahead(optimizer, 7, 1)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=train_steps * 0.1, t_total=train_steps)\n",
    "    return optimizer, scheduler\n",
    "def to_predict(model, dataloader,output_model, with_labels=False):\n",
    "    \n",
    "    # load model\n",
    "    checkpoint = torch.load(output_model, map_location='cuda')\n",
    "#     print(checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "\n",
    "    print('-----Testing-----')\n",
    "\n",
    "    pred_label =np.zeros((len(test),29))\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            predict1,predict2 = model(batch[0], batch[1])\n",
    "            predict1 = predict1.detach().cpu().numpy()\n",
    "            predict2 = predict2.detach().cpu().numpy()\n",
    "            predict=np.concatenate([predict1,predict2],axis=-1)\n",
    "            pred_label[i*batch_size:(i+1)*batch_size]=predict\n",
    "    return pred_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "underlying-excitement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T13:32:31.563615Z",
     "start_time": "2021-05-10T13:32:31.457868Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../tcdata/train.csv',header=None)\n",
    "test=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "\n",
    "# test=train.iloc[-2000:].copy().reset_index(drop=True)\n",
    "# train=train.iloc[:-2000]\n",
    "\n",
    "# test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "model_path='../model_weight/xlnet/'\n",
    "output_model='../tmp/xlNet.pth'\n",
    "batch_size=32\n",
    "# 合并训练集与测试集 制作特征\n",
    "for i in range(1,3):\n",
    "    train[i]=train[i].apply(lambda x:x.replace('|','').strip())\n",
    "for i in range(1,2):\n",
    "    test[i]=test[i].apply(lambda x:x.replace('|','').strip())\n",
    "train.columns=['idx','sentence','label1','label2']\n",
    "test.columns=['idx','sentence']\n",
    "# test.columns=['idx','sentence','label1','label2']\n",
    "\n",
    "tokenizer=BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "config=XLNetConfig.from_pretrained(model_path,num_labels=17,hidden_dropout_prob=0.2) # config.output_attentions=True\n",
    "config.hidden_dropout_prob=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "signal-coffee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T13:32:51.163158Z",
     "start_time": "2021-05-10T13:32:51.146111Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_df,val_df,test_oof):\n",
    "    \n",
    "        ###--------------------\n",
    "    early_stop=0\n",
    "    print(\"Reading training data...\")\n",
    "    train_set = CustomDataset(train_df, maxlen=128,tokenizer=tokenizer)\n",
    "    train_loader = Data.DataLoader(train_set, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "\n",
    "    print(\"Reading validation data...\")\n",
    "    val_set = CustomDataset(val_df, maxlen=128, tokenizer=tokenizer)\n",
    "    val_loader = Data.DataLoader(val_set, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "\n",
    "    test_set = CustomDataset(test, maxlen=128, tokenizer=tokenizer,with_labels=False)\n",
    "    test_loader = Data.DataLoader(test_set, batch_size=batch_size, num_workers=5, shuffle=False)\n",
    "    # 准备模型\n",
    "    model=NeZhaForSequenceClassification(config=config,model_name=model_path,num_labels1=17,num_labels2=12)\n",
    "    ### 训练\n",
    "    model.to(device)\n",
    "    fgm = FGM(model)\n",
    "    train_num = len(train_set)\n",
    "    train_steps = int(train_num * epochs / batch_size) + 1\n",
    "\n",
    "    optimizer, scheduler = build_optimizer(model, train_steps, learning_rate=2e-5)\n",
    "    print('-----Training-----')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        print('Epoch', epoch)\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            loss, predict1,predict2 = model(batch[0], batch[1], batch[2],batch[3])\n",
    "            if i % 50 == 0:\n",
    "                print(i, loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # 对抗训练\n",
    "            fgm.attack()\n",
    "            loss_adv, _,_  = model(batch[0], batch[1], batch[2],batch[3])\n",
    "            loss_adv.backward()\n",
    "            fgm.restore()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "   \n",
    "    #         if i % 20 == 0:\n",
    "    #             eval(model, optimizer, val_loader, output_model='./runs/nezha1.pth')\n",
    "       \n",
    "        early_stop+=evals(model, optimizer, val_loader, output_model='../tmp/xlnet_v2_{}'.format(fold_s))\n",
    "        if early_stop==2:\n",
    "            break\n",
    "\n",
    "#     test_oof.append(to_predict(model, test_loader,output_model, with_labels=False))\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return test_oof   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "descending-report",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T13:32:52.659717Z",
     "start_time": "2021-05-10T13:32:52.655803Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "statewide-lover",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T14:17:32.555493Z",
     "start_time": "2021-05-10T13:32:53.626554Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training-----\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.594738483428955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-0c15f11d7ceb>:393: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370117127/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "  9%|▉         | 50/547 [00:51<08:29,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.44041651487350464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [01:42<07:37,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.3043021559715271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [02:35<07:08,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.18678994476795197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [03:28<05:58,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.16192379593849182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 250/547 [04:21<05:22,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.13544520735740662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [05:14<04:28,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.08040349185466766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 350/547 [06:08<03:24,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 0.08857680857181549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [07:01<02:40,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.08177413046360016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 450/547 [07:55<01:41,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.10394725203514099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [08:48<00:49,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.08729692548513412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [09:39<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.07745061970398395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 1\n",
      "0 0.0920451432466507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/547 [00:53<08:57,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.052885882556438446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [01:47<07:55,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.0743522047996521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [02:41<07:15,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.07262754440307617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [03:35<06:10,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.03560727834701538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 250/547 [04:28<05:21,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.05430780351161957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [05:22<04:30,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.07251229137182236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 350/547 [06:16<03:36,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 0.05653151124715805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [07:10<02:39,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.03844248875975609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 450/547 [08:04<01:43,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.06310568004846573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [08:58<00:50,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.0516413152217865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [09:48<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.060765899865168935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 2\n",
      "0 0.047778524458408356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/547 [00:53<08:51,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.056427981704473495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [01:47<08:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.048545755445957184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [02:41<07:08,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.07957228273153305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [03:35<06:12,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.06383830308914185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 250/547 [04:30<05:24,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.01974083110690117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [05:24<04:24,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.04441862180829048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 350/547 [06:17<03:34,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 0.06788873672485352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [07:11<02:38,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.026682596653699875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 450/547 [08:05<01:44,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.015764515846967697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [08:59<00:50,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.04674725979566574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [09:50<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.05237239672319044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 3\n",
      "0 0.03676239401102066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/547 [00:54<09:04,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.02995159849524498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [01:48<08:06,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.045486416667699814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [02:42<07:10,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.04284702241420746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [03:35<06:14,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.030424639582633972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 250/547 [04:29<05:17,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.03350168094038963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [05:24<04:30,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.036580853164196014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 350/547 [06:17<03:32,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 0.024643542245030403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [07:11<02:36,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.009956941939890385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 450/547 [08:06<01:46,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.03512725606560707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [09:00<00:51,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.042021363973617554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [09:50<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.05016708290322295\n",
      "The best model has been saved\n",
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training-----\n",
      "Epoch 0\n",
      "0 1.5282504558563232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/547 [00:53<09:02,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.3868294060230255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [01:47<07:56,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.29297590255737305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [02:41<07:04,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.21367566287517548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [03:34<06:10,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.21242937445640564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 225/547 [04:02<05:47,  1.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b3f392fdc6b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mval_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mfold_s\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d8f1255d2b7a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_df, val_df, test_oof)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mfgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mloss_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mfgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_fold=KFold(8,shuffle=True,random_state=10888)\n",
    "test_oof=[]\n",
    "epochs = 5\n",
    "fold_s=0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for trn_idx,val_idx in n_fold.split(train):\n",
    "    if fold_s>1:\n",
    "        \n",
    "        train_df=train.iloc[trn_idx].reset_index(drop=True)\n",
    "        val_df=train.iloc[val_idx].reset_index(drop=True)\n",
    "        best_score = float('inf')\n",
    "        train_model(train_df,val_df,test_oof)\n",
    "    fold_s+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alnet   al_new    drop=0.2          ->(alnet)drop0.1->(alnet_new)drop0.1 ->drop0.2(alnet) ->drop0.1(alnet len=100) ->change drop ->sanfen\n",
    "# 0.0701-> 0.066   - 0.068  ->0.0689  ->0.06719 ->        0.068             ->0.685          >0.0687                ->0.0699        ->0.065\n",
    "# 0.0473 ->  0.0488->0.049 ->0.0523   ->                                                                              ->0.0552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "              \n",
    "            3fen drop0.1      3fen drop0.2  4fen\n",
    "0.07   0.705   0.064           0.0657        0.0668\n",
    "0.051  0.0502   0.048          0.0502        0.047\n",
    "0.043  0.0397   0.0403        0.04002       0.0394\n",
    "0.040  0.036    0.037          0.0362       0.038\n",
    "               0.035        0.358         0.356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "sub['report_ID']=test[0]\n",
    "sub['Prediction']=[ '|'+' '.join(['%.12f'%j for j in i]) for i in test_oof ]\n",
    "sub.to_csv('../result.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
