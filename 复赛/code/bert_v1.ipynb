{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stuffed-charm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:38:55.189773Z",
     "start_time": "2021-05-06T01:38:55.122199Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer,AutoModelForPreTraining\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,AutoModel,BertTokenizer\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n",
    "from model.modeling_nezha import NeZhaForSequenceClassification,NeZhaPreTrainedModel,NeZhaModel,NeZhaForTokenClassification\n",
    "from model.configuration_nezha import NeZhaConfig\n",
    "import  torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertOutput,\n",
    "    BertPooler,\n",
    "    BertSelfOutput,\n",
    "    BertIntermediate,\n",
    "    BertOnlyMLMHead,\n",
    "    BertOnlyNSPHead,\n",
    "    BertPreTrainingHeads,\n",
    "    BERT_START_DOCSTRING,\n",
    "    BERT_INPUTS_DOCSTRING,\n",
    ")\n",
    "from transformers import BertConfig,BertTokenizerFast,BertModel\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "class NeZhaForSequenceClassification(NeZhaPreTrainedModel):\n",
    "    def __init__(self, config,model_name,num_labels1,num_labels2):\n",
    "        super().__init__(config)\n",
    "        self.num_labels1 = num_labels1\n",
    "        self.num_labels2=num_labels2\n",
    "        self.bert = BertModel(config).from_pretrained(model_name)\n",
    "        self.attn1=Attn(config.hidden_size)\n",
    "        self.attn2=Attn(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier1 = nn.Linear(config.hidden_size, self.num_labels1)\n",
    "        self.classifier2 = nn.Linear(config.hidden_size, self.num_labels2)\n",
    "        self.predict=nn.Sigmoid()\n",
    "#         self.init_weights()\n",
    "#         if True:\n",
    "#             for p in self.bert.parameters(): # 冻结所有bert层\n",
    "#                 p.requires_grad = False\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            labels1=None,\n",
    "        labels2=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "    Returns:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        import torch\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        att1=self.attn1(outputs[0])\n",
    "        att2=self.attn2(outputs[0])\n",
    "        pooled_output1 = self.dropout(att1)\n",
    "        pooled_output2 = self.dropout(att2)\n",
    "        logits1 = self.classifier1(pooled_output1)\n",
    "        logits2 = self.classifier2(pooled_output2)\n",
    "        predict1=self.predict(logits1)\n",
    "        predict2=self.predict(logits2)\n",
    "        outputs = (predict1,predict2) + outputs[2:]  # add hidden states and attention if they are here\n",
    "#         print('label:',labels)\n",
    "#         print('input_ids:',input_ids)\n",
    "#         print('attention_mas:',attention_mask)\n",
    "        if labels1 is not None:\n",
    "            loss_fct = nn.BCELoss()\n",
    "#                 print(logits.view(-1, self.num_labels))\n",
    "#                 print(labels.view(-1, self.num_labels))\n",
    "            loss1 = loss_fct(predict1.view(-1, self.num_labels1), labels1.view(-1, self.num_labels1))\n",
    "            loss2 = loss_fct(predict2.view(-1, self.num_labels2), labels2.view(-1, self.num_labels2))\n",
    "            loss=loss1+loss2\n",
    "            outputs = (loss,) + outputs\n",
    "#         print(outputs)\n",
    "        return outputs  # (loss), predict1,predict2, (hidden_states), (attentions)\n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size,1)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :param src_len:\n",
    "            used for masking. NoneType or tensor in shape (B) indicating sequence length\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''   \n",
    "        att=self.attn(x)\n",
    "        att=F.tanh(att)\n",
    "        att=F.softmax(att,1)\n",
    "        att_x=att*x\n",
    "        return att_x.sum(1)\n",
    "import torch.utils.data as Data\n",
    "class CustomDataset(Data.Dataset):\n",
    "    def __init__(self, data, maxlen,tokenizer,with_labels=True, model_name='bert-base-chinese'):\n",
    "        self.data = data  # pandas dataframe\n",
    "\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = tokenizer#AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def get_label(self,x,num):\n",
    "        label=[0]*num\n",
    "       \n",
    "        x=x.strip().split(' ')\n",
    "\n",
    "        for l in x:              \n",
    "            if l and l!='nan':\n",
    "                label[int(l)]=1\n",
    "        return label\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        sent = str(self.data.loc[index, 'sentence'])\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent,\n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,       # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
    "#         print(encoded_pair['input_ids'])\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "#         token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            label1 = torch.Tensor(self.get_label(str(self.data.loc[index, 'label1']),17))\n",
    "            label2 = torch.Tensor(self.get_label(str(self.data.loc[index, 'label2']),12))\n",
    "            return token_ids, attn_masks,label1,label2\n",
    "        else:\n",
    "            return token_ids, attn_masks\n",
    "from sklearn.utils import shuffle as reset\n",
    "def train_test_split(data_df, test_size=0.2, shuffle=True, random_state=None):\n",
    "    if shuffle:\n",
    "        data_df = reset(data_df, random_state=random_state)\n",
    "\n",
    "    train = data_df[int(len(data_df)*test_size):].reset_index(drop = True)\n",
    "    test  = data_df[:int(len(data_df)*test_size)].reset_index(drop = True)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "from torch.nn.functional import cross_entropy,binary_cross_entropy\n",
    "\n",
    "\n",
    "def evals(model, optimizer, validation_dataloader,output_model = './train_class/model.pth'):\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            predict1,predict2 = model(batch[0], batch[1])\n",
    "            predict1,predict2 = predict1.detach().cpu(),predict2.detach().cpu()\n",
    "            label_ids1,label_ids2 = batch[2].cpu(),batch[3].cpu()\n",
    "            \n",
    "            tmp_eval_accuracy = binary_cross_entropy(predict1, label_ids1.float()).item()+binary_cross_entropy(predict2, label_ids2.float()).item()\n",
    "            \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation mlogloss: {}\".format(eval_accuracy / nb_eval_steps))\n",
    "    global best_score\n",
    "    if best_score > eval_accuracy / nb_eval_steps:\n",
    "        best_score = eval_accuracy / nb_eval_steps\n",
    "        save(model, optimizer,output_model)\n",
    "        return 0\n",
    "    return 1\n",
    "def save(model, optimizer,output_model):\n",
    "    # save\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, output_model)\n",
    "    print('The best model has been saved')\n",
    "def flat_accuracy(preds, labels):\n",
    "#     print(preds,labels)\n",
    "    return -np.mean(labels*np.log(preds+1.e-7)+(1-labels)*np.log(preds+1.e-7))*10\n",
    "\n",
    "# 对抗训练\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "    def attack(self, epsilon=1.0, emb_name='word_emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "    def restore(self, emb_name='word_emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        \n",
    "from collections import defaultdict\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "\n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "\n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "class WarmupLinearSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then linear decay.\n",
    "        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n",
    "        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n",
    "        Linearly decreases the multiplicative variable from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1, self.warmup_steps))\n",
    "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        correct_bias=correct_bias)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                step_size = group['lr']\n",
    "                if group['correct_bias']:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state['step']\n",
    "                    bias_correction2 = 1.0 - beta2 ** state['step']\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                if group['weight_decay'] > 0.0:\n",
    "                    p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n",
    "        return loss\n",
    "def build_optimizer(model, train_steps, learning_rate):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, correct_bias=False, eps=1e-8)\n",
    "    optimizer = Lookahead(optimizer, 5, 1)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=train_steps * 0.1, t_total=train_steps)\n",
    "    return optimizer, scheduler\n",
    "def to_predict(model, dataloader,output_model, with_labels=False):\n",
    "    \n",
    "    # load model\n",
    "    checkpoint = torch.load(output_model, map_location='cuda')\n",
    "#     print(checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "\n",
    "    print('-----Testing-----')\n",
    "\n",
    "    pred_label =np.zeros((len(test),29))\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            predict1,predict2 = model(batch[0], batch[1])\n",
    "            predict1 = predict1.detach().cpu().numpy()\n",
    "            predict2 = predict2.detach().cpu().numpy()\n",
    "            predict=np.concatenate([predict1,predict2],axis=-1)\n",
    "            pred_label[i*batch_size:(i+1)*batch_size]=predict\n",
    "    return pred_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "smoking-climb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:38:55.467550Z",
     "start_time": "2021-05-06T01:38:55.357352Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../tcdata/train.csv',header=None)\n",
    "test=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "\n",
    "# test=train.iloc[-2000:].copy().reset_index(drop=True)\n",
    "# train=train.iloc[:-2000]\n",
    "\n",
    "# test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "model_path='../model_weight/bert/'\n",
    "output_model='../tmp/bert.pth'\n",
    "batch_size=32\n",
    "# 合并训练集与测试集 制作特征\n",
    "for i in range(1,3):\n",
    "    train[i]=train[i].apply(lambda x:x.replace('|','').strip())\n",
    "for i in range(1,2):\n",
    "    test[i]=test[i].apply(lambda x:x.replace('|','').strip())\n",
    "train.columns=['idx','sentence','label1','label2']\n",
    "test.columns=['idx','sentence']\n",
    "# test.columns=['idx','sentence','label1','label2']\n",
    "\n",
    "tokenizer=BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "config=BertConfig.from_pretrained(model_path,num_labels=17,hidden_dropout_prob=0.1) # config.output_attentions=True\n",
    "config.hidden_dropout_prob=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "straight-syndrome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:38:55.685245Z",
     "start_time": "2021-05-06T01:38:55.669393Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_df,val_df,test_oof):\n",
    "    \n",
    "        ###--------------------\n",
    "    early_stop=0\n",
    "    print(\"Reading training data...\")\n",
    "    train_set = CustomDataset(train_df, maxlen=128,tokenizer=tokenizer)\n",
    "    train_loader = Data.DataLoader(train_set, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "\n",
    "    print(\"Reading validation data...\")\n",
    "    val_set = CustomDataset(val_df, maxlen=128, tokenizer=tokenizer)\n",
    "    val_loader = Data.DataLoader(val_set, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "\n",
    "    test_set = CustomDataset(test, maxlen=128, tokenizer=tokenizer,with_labels=False)\n",
    "    test_loader = Data.DataLoader(test_set, batch_size=batch_size, num_workers=5, shuffle=False)\n",
    "    # 准备模型\n",
    "    model=NeZhaForSequenceClassification(config=config,model_name=model_path,num_labels1=17,num_labels2=12)\n",
    "    ### 训练\n",
    "    model.to(device)\n",
    "    fgm = FGM(model)\n",
    "    train_num = len(train_set)\n",
    "    train_steps = int(train_num * epochs / batch_size) + 1\n",
    "\n",
    "    optimizer, scheduler = build_optimizer(model, train_steps, learning_rate=6e-5)\n",
    "    print('-----Training-----')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        print('Epoch', epoch)\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            loss, predict1,predict2 = model(batch[0], batch[1], batch[2],batch[3])\n",
    "            if i % 50 == 0:\n",
    "                print(i, loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # 对抗训练\n",
    "            fgm.attack()\n",
    "            loss_adv, _,_  = model(batch[0], batch[1], batch[2],batch[3])\n",
    "            loss_adv.backward()\n",
    "            fgm.restore()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "   \n",
    "    #         if i % 20 == 0:\n",
    "    #             eval(model, optimizer, val_loader, output_model='./runs/nezha1.pth')\n",
    "       \n",
    "        early_stop+=evals(model, optimizer, val_loader, output_model=output_model)\n",
    "        if early_stop==2:\n",
    "            break\n",
    "\n",
    "    test_oof.append(to_predict(model, test_loader,output_model, with_labels=False))\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return test_oof   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "opened-kidney",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T02:06:13.045531Z",
     "start_time": "2021-05-06T01:38:56.172744Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../model_weight/bert/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training-----\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3908965587615967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-bf94a94bf2f7>:381: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370117127/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "  9%|▉         | 50/547 [00:24<04:01,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.3625543415546417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [00:49<03:42,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.306037962436676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [01:14<03:19,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.1908472329378128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [01:40<02:55,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.21352708339691162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 250/547 [02:06<02:41,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.16700202226638794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [02:34<02:21,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.12694424390792847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 350/547 [03:04<02:01,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 0.13708218932151794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [03:36<01:32,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.09468851238489151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 450/547 [04:08<01:06,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.07340187579393387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [04:43<00:31,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.07787097245454788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [05:22<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.07766225971753084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 1\n",
      "0 0.06797514855861664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/547 [00:37<06:04,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.05992788448929787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [01:23<06:15,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.08078087121248245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [02:11<07:34,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.07217096537351608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [02:58<06:01,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.10543860495090485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 250/547 [03:45<03:21,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.06236641854047775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [04:40<04:59,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.05149432271718979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 350/547 [05:31<03:57,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 0.06978238373994827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [06:23<02:30,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.06067626550793648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 450/547 [07:17<01:52,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.06013014540076256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [08:17<00:52,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.05723525583744049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [09:09<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.05485029469090927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 2\n",
      "0 0.04070832207798958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/547 [00:50<09:23,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.03763046860694885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [01:48<09:58,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.027600131928920746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/547 [02:48<08:11,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.04832497239112854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [03:46<07:49,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.0363018698990345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 250/547 [04:43<04:30,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.022187354043126106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [05:41<04:03,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.042010314762592316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 350/547 [06:40<03:41,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 0.029389146715402603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [07:36<01:56,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.04637901857495308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 450/547 [08:31<01:14,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.03394341096282005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [09:29<00:39,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.02750646136701107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [10:26<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.04313594408191835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 3\n",
      "0 0.014517219737172127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/547 [00:51<10:11,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.020644279196858406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 70/547 [01:16<08:39,  1.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-789436ee2014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-38854974f4bf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_df, val_df, test_oof)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# 对抗训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_fold=KFold(8,shuffle=True,random_state=1080)\n",
    "test_oof=[]\n",
    "epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for trn_idx,val_idx in n_fold.split(train):\n",
    "    train_df=train.iloc[trn_idx].reset_index(drop=True)\n",
    "    val_df=train.iloc[val_idx].reset_index(drop=True)\n",
    "    best_score = float('inf')\n",
    "    train_model(train_df,val_df,test_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "swiss-ranch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:38:47.506699Z",
     "start_time": "2021-05-06T01:38:47.501678Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-099d2d28035e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-099d2d28035e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    0.052\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# 0.1085 0.0792  0.076   0.077\n",
    "                0.052    0.0548\n",
    "                         0.043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "level-american",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:38:48.678679Z",
     "start_time": "2021-05-06T01:38:48.388564Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_oof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-53e3ae628200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtest_oof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_oof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_oof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_oof' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_oof)):\n",
    "    test_oof[i]=test_oof[i]-test_oof[i].min(axis=1,keepdims=True)\n",
    "test_oof=np.mean(test_oof,axis=0)    \n",
    "test_oof=test_oof-test_oof.min(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "sub['report_ID']=test[0]\n",
    "sub['Prediction']=[ '|'+' '.join(['%.12f'%j for j in i]) for i in test_oof ]\n",
    "sub.to_csv('../result.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
