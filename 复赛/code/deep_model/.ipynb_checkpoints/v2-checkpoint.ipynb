{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unable-israel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:03.008312Z",
     "start_time": "2021-04-15T10:46:02.990369Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import Model\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Embedding, Dense, CuDNNLSTM,CuDNNGRU, Bidirectional,SpatialDropout1D,Input,\\\n",
    "GlobalAveragePooling1D,GlobalMaxPooling1D,Conv1D,concatenate,Dropout,Activation,BatchNormalization,Concatenate,Add,\\\n",
    "MaxPooling1D,Flatten,AveragePooling1D\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "import gensim, logging\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.model_selection import KFold,StratifiedShuffleSplit,StratifiedKFold\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pregnant-ozone",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:03.293717Z",
     "start_time": "2021-04-15T10:46:03.187907Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../tcdata/train.csv',header=None)\n",
    "test=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "# test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "data=pd.concat([train,test],axis=0)\n",
    "data[1]=data[1].apply(lambda x:x.strip().replace('|',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "noble-manhattan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:03.490101Z",
     "start_time": "2021-04-15T10:46:03.396863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noncase label1: 0\n",
      "noncase label2: 8247\n"
     ]
    }
   ],
   "source": [
    "## 制作标签\n",
    "### 创建训练集标签 \n",
    "train_labels1=np.zeros((len(train),17),dtype='int8')\n",
    "noncase=0\n",
    "for cnt,i in enumerate(train[2]):\n",
    "    if i:\n",
    "        lab=[int(x.replace('|','').strip()) for x in i.split(' ') if x and x!='|']\n",
    "        for l in lab:\n",
    "            train_labels1[cnt,l]=1\n",
    "    else:\n",
    "        noncase+=1\n",
    "print('noncase label1:',noncase)\n",
    "#----------------------------------\n",
    "noncase=0\n",
    "train_labels2=np.zeros((len(train),12),dtype='int8')\n",
    "for cnt,i in enumerate(train[3]):\n",
    "    if pd.notna(i):\n",
    "        lab=[int(x.replace('|','').strip()) for x in i.split(' ') if x and x!='|']\n",
    "        for l in lab:\n",
    "            train_labels2[cnt,l]=1\n",
    "    else:\n",
    "        noncase+=1\n",
    "print('noncase label2:',noncase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "specified-hebrew",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:03.609658Z",
     "start_time": "2021-04-15T10:46:03.603909Z"
    }
   },
   "outputs": [],
   "source": [
    "cate_num=train_labels1.sum(1)+train_labels2.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "floral-thickness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:04.496910Z",
     "start_time": "2021-04-15T10:46:04.492528Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels=np.concatenate([train_labels1,train_labels2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "english-burlington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:02:19.297605Z",
     "start_time": "2021-04-15T06:02:19.227226Z"
    }
   },
   "outputs": [],
   "source": [
    "## 生成全部的训练文本\n",
    "data1=pd.read_csv('../tcdata/track1_round1_train_20210222.csv',header=None)\n",
    "data2=pd.read_csv('../tcdata/track1_round1_testA_20210222.csv',header=None)\n",
    "data3=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "text_data=pd.concat([train,data1,data2,data3],axis=0)\n",
    "text_data[1]=text_data[1].apply(lambda x:x.strip().replace('|',''))\n",
    "text_data[1].to_csv('../tmp/all_data.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "overhead-plenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:07:16.368462Z",
     "start_time": "2021-04-15T06:07:16.288461Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data=pd.read_csv('../tmp/all_data.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "certified-afternoon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:24.265950Z",
     "start_time": "2021-04-15T10:46:24.262281Z"
    }
   },
   "outputs": [],
   "source": [
    "# w2v=Word2Vec(text_data[0].apply(lambda x:x.split(' ')).tolist(),size=128, window=8, iter=30, min_count=2,\n",
    "#                      sg=1, sample=0.002, workers=6 , seed=1017)\n",
    "\n",
    "# w2v.wv.save_word2vec_format('../tmp/w2v_128.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dental-thing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:26.065184Z",
     "start_time": "2021-04-15T10:46:24.804628Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False, char_level=False, split=' ')\n",
    "tokenizer.fit_on_texts(data[1].tolist())\n",
    "seq = tokenizer.texts_to_sequences(data[1].tolist())\n",
    "# 分训练和测试集合\n",
    "seq = pad_sequences(seq, maxlen=128, value=0)\n",
    "train_seq=np.asarray(seq[:len(train)])\n",
    "test_seq=seq[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "governmental-netscape",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:26.689666Z",
     "start_time": "2021-04-15T10:46:26.540104Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 18:46:26,544 : INFO : loading projection weights from ../tmp/w2v_128.txt\n",
      "2021-04-15 18:46:26,660 : INFO : loaded (859, 128) matrix from ../tmp/w2v_128.txt\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(859, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix=np.zeros((len(tokenizer.word_index)+1,128))\n",
    "w2v=gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        '../tmp/w2v_128.txt', binary=False)\n",
    "\n",
    "for word in tokenizer.word_index:\n",
    "    if word not in w2v.wv.vocab:\n",
    "        continue\n",
    "    embedding_matrix[tokenizer.word_index[word]] = w2v[word]\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "compact-bearing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:55:22.287152Z",
     "start_time": "2021-04-15T10:55:22.244287Z"
    }
   },
   "outputs": [],
   "source": [
    "def NN_huaweiv1(maxlen,embedding_matrix=None,class_num1=17,class_num2=12):    \n",
    "    emb_layer = Embedding(\n",
    "       embedding_matrix.shape[0], embedding_matrix.shape[1],input_length=maxlen,weights=[embedding_matrix],trainable=False,\n",
    "    )\n",
    "    seq1 = Input(shape=(maxlen,)) \n",
    "    \n",
    "    x1 = emb_layer(seq1)\n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "    lstm_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    gru_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    cnn1d_layer=Conv1D(64, kernel_size=3, padding=\"same\", kernel_initializer=\"he_uniform\")\n",
    "    x1 = sdrop(x1)\n",
    "    lstm1 = lstm_layer(x1)\n",
    "    gru1 = gru_layer(lstm1)\n",
    "    att_1 = Attention(maxlen)(lstm1)\n",
    "    att_2 = Attention(maxlen)(gru1)\n",
    "    cnn1 = cnn1d_layer(lstm1)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()\n",
    "    max_pool = GlobalMaxPooling1D()\n",
    "\n",
    "    x1=concatenate([att_1,att_2,Attention(maxlen)(cnn1),avg_pool(cnn1),max_pool(cnn1)])\n",
    "\n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(128)(x1))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(64)(x)))\n",
    "    pred1_d = Dense(class_num1)(x)\n",
    "    pred1=Activation(activation='sigmoid',name='pred1')(pred1_d)\n",
    "    \n",
    "    y=concatenate([x1,x])\n",
    "    y = Activation(activation=\"relu\")(BatchNormalization()(Dense(64)(x)))\n",
    "    pred2_d=Dense(class_num2)(y)\n",
    "    pred2=Activation(activation='sigmoid',name='pred2')(pred2_d)\n",
    "    \n",
    "    z=Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(128)(x1))))\n",
    "    z=concatenate([pred1_d,pred2_d,z])\n",
    "    pred3= Dense(class_num1+class_num2, activation='sigmoid',name='pred3')(z)\n",
    "    model = Model(inputs=seq1, outputs=[pred1,pred2,pred3])\n",
    "    return model\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "#         print('-------------',type(input_shape))\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return -K.mean(y_true*K.log(y_pred+1.e-7)+(1-y_true)*K.log(1-y_pred+1.e-7))*10\n",
    "class Lookahead(object):\n",
    "    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=5, alpha=0.5):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.count = 0\n",
    "\n",
    "    def inject(self, model):\n",
    "        \"\"\"Inject the Lookahead algorithm for the given model.\n",
    "        The following code is modified from keras's _make_train_function method.\n",
    "        See: https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497\n",
    "        \"\"\"\n",
    "        if not hasattr(model, 'train_function'):\n",
    "            raise RuntimeError('You must compile your model before using it.')\n",
    "\n",
    "        model._check_trainable_weights_consistency()\n",
    "\n",
    "        if model.train_function is None:\n",
    "            inputs = (model._feed_inputs +\n",
    "                      model._feed_targets +\n",
    "                      model._feed_sample_weights)\n",
    "            if model._uses_dynamic_learning_phase():\n",
    "                inputs += [K.learning_phase()]\n",
    "            fast_params = model._collected_trainable_weights\n",
    "\n",
    "            with K.name_scope('training'):\n",
    "                with K.name_scope(model.optimizer.__class__.__name__):\n",
    "                    training_updates = model.optimizer.get_updates(\n",
    "                        params=fast_params,\n",
    "                        loss=model.total_loss)\n",
    "                    slow_params = [K.variable(p) for p in fast_params]\n",
    "                fast_updates = (model.updates +\n",
    "                                training_updates +\n",
    "                                model.metrics_updates)\n",
    "\n",
    "                slow_updates, copy_updates = [], []\n",
    "                for p, q in zip(fast_params, slow_params):\n",
    "                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n",
    "                    copy_updates.append(K.update(p, q))\n",
    "\n",
    "                # Gets loss and metrics. Updates weights at each call.\n",
    "                fast_train_function = K.function(\n",
    "                    inputs,\n",
    "                    [model.total_loss] + model.metrics_tensors,\n",
    "                    updates=fast_updates,\n",
    "                    name='fast_train_function',\n",
    "                    **model._function_kwargs)\n",
    "\n",
    "                def F(inputs):\n",
    "                    self.count += 1\n",
    "                    R = fast_train_function(inputs)\n",
    "                    if self.count % self.k == 0:\n",
    "                        K.batch_get_value(slow_updates)\n",
    "                        K.batch_get_value(copy_updates)\n",
    "                    return R\n",
    "                \n",
    "                model.train_function = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "accurate-butler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:57:50.168402Z",
     "start_time": "2021-04-15T10:55:24.241293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=8.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Build model...\n",
      "Train...\n",
      "Train on 17500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      "17500/17500 [==============================] - 13s 736us/step - loss: 1.2810 - pred1_loss: 0.4855 - pred2_loss: 0.4720 - pred3_loss: 0.3236 - pred1_acc: 0.7807 - pred1_mean_pred: 4.8551 - pred2_acc: 0.8024 - pred2_mean_pred: 4.7195 - pred3_acc: 0.8706 - pred3_mean_pred: 3.2358 - val_loss: 0.5379 - val_pred1_loss: 0.2172 - val_pred2_loss: 0.1795 - val_pred3_loss: 0.1413 - val_pred1_acc: 0.9411 - val_pred1_mean_pred: 2.1721 - val_pred2_acc: 0.9642 - val_pred2_mean_pred: 1.7945 - val_pred3_acc: 0.9541 - val_pred3_mean_pred: 1.4126\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53792, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 2/100\n",
      "17500/17500 [==============================] - 7s 382us/step - loss: 0.4895 - pred1_loss: 0.2093 - pred2_loss: 0.1617 - pred3_loss: 0.1184 - pred1_acc: 0.9454 - pred1_mean_pred: 2.0932 - pred2_acc: 0.9707 - pred2_mean_pred: 1.6174 - pred3_acc: 0.9588 - pred3_mean_pred: 1.1844 - val_loss: 0.3860 - val_pred1_loss: 0.1659 - val_pred2_loss: 0.1151 - val_pred3_loss: 0.1050 - val_pred1_acc: 0.9512 - val_pred1_mean_pred: 1.6592 - val_pred2_acc: 0.9718 - val_pred2_mean_pred: 1.1510 - val_pred3_acc: 0.9613 - val_pred3_mean_pred: 1.0498\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53792 to 0.38601, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 3/100\n",
      "17500/17500 [==============================] - 7s 388us/step - loss: 0.3477 - pred1_loss: 0.1524 - pred2_loss: 0.1003 - pred3_loss: 0.0950 - pred1_acc: 0.9510 - pred1_mean_pred: 1.5239 - pred2_acc: 0.9758 - pred2_mean_pred: 1.0030 - pred3_acc: 0.9643 - pred3_mean_pred: 0.9504 - val_loss: 0.3107 - val_pred1_loss: 0.1380 - val_pred2_loss: 0.0885 - val_pred3_loss: 0.0843 - val_pred1_acc: 0.9555 - val_pred1_mean_pred: 1.3802 - val_pred2_acc: 0.9787 - val_pred2_mean_pred: 0.8846 - val_pred3_acc: 0.9668 - val_pred3_mean_pred: 0.8426\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38601 to 0.31074, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 4/100\n",
      "17500/17500 [==============================] - 7s 386us/step - loss: 0.2883 - pred1_loss: 0.1284 - pred2_loss: 0.0767 - pred3_loss: 0.0832 - pred1_acc: 0.9548 - pred1_mean_pred: 1.2841 - pred2_acc: 0.9794 - pred2_mean_pred: 0.7668 - pred3_acc: 0.9676 - pred3_mean_pred: 0.8320 - val_loss: 0.2635 - val_pred1_loss: 0.1178 - val_pred2_loss: 0.0677 - val_pred3_loss: 0.0781 - val_pred1_acc: 0.9560 - val_pred1_mean_pred: 1.1777 - val_pred2_acc: 0.9821 - val_pred2_mean_pred: 0.6766 - val_pred3_acc: 0.9682 - val_pred3_mean_pred: 0.7806\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31074 to 0.26350, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 5/100\n",
      "17500/17500 [==============================] - 7s 386us/step - loss: 0.2528 - pred1_loss: 0.1139 - pred2_loss: 0.0637 - pred3_loss: 0.0752 - pred1_acc: 0.9574 - pred1_mean_pred: 1.1391 - pred2_acc: 0.9820 - pred2_mean_pred: 0.6368 - pred3_acc: 0.9703 - pred3_mean_pred: 0.7521 - val_loss: 0.2306 - val_pred1_loss: 0.1051 - val_pred2_loss: 0.0581 - val_pred3_loss: 0.0675 - val_pred1_acc: 0.9600 - val_pred1_mean_pred: 1.0508 - val_pred2_acc: 0.9828 - val_pred2_mean_pred: 0.5806 - val_pred3_acc: 0.9728 - val_pred3_mean_pred: 0.6747\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26350 to 0.23060, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 6/100\n",
      "17500/17500 [==============================] - 7s 384us/step - loss: 0.2267 - pred1_loss: 0.1040 - pred2_loss: 0.0552 - pred3_loss: 0.0675 - pred1_acc: 0.9597 - pred1_mean_pred: 1.0402 - pred2_acc: 0.9837 - pred2_mean_pred: 0.5518 - pred3_acc: 0.9733 - pred3_mean_pred: 0.6752 - val_loss: 0.2070 - val_pred1_loss: 0.0978 - val_pred2_loss: 0.0498 - val_pred3_loss: 0.0594 - val_pred1_acc: 0.9630 - val_pred1_mean_pred: 0.9781 - val_pred2_acc: 0.9851 - val_pred2_mean_pred: 0.4975 - val_pred3_acc: 0.9779 - val_pred3_mean_pred: 0.5944\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23060 to 0.20700, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 7/100\n",
      "17500/17500 [==============================] - 7s 389us/step - loss: 0.2007 - pred1_loss: 0.0939 - pred2_loss: 0.0493 - pred3_loss: 0.0576 - pred1_acc: 0.9640 - pred1_mean_pred: 0.9386 - pred2_acc: 0.9851 - pred2_mean_pred: 0.4929 - pred3_acc: 0.9789 - pred3_mean_pred: 0.5758 - val_loss: 0.1928 - val_pred1_loss: 0.0917 - val_pred2_loss: 0.0473 - val_pred3_loss: 0.0538 - val_pred1_acc: 0.9666 - val_pred1_mean_pred: 0.9168 - val_pred2_acc: 0.9860 - val_pred2_mean_pred: 0.4726 - val_pred3_acc: 0.9804 - val_pred3_mean_pred: 0.5382\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20700 to 0.19277, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 8/100\n",
      "17500/17500 [==============================] - 7s 388us/step - loss: 0.1782 - pred1_loss: 0.0831 - pred2_loss: 0.0444 - pred3_loss: 0.0507 - pred1_acc: 0.9699 - pred1_mean_pred: 0.8312 - pred2_acc: 0.9865 - pred2_mean_pred: 0.4438 - pred3_acc: 0.9821 - pred3_mean_pred: 0.5073 - val_loss: 0.1697 - val_pred1_loss: 0.0787 - val_pred2_loss: 0.0449 - val_pred3_loss: 0.0461 - val_pred1_acc: 0.9761 - val_pred1_mean_pred: 0.7869 - val_pred2_acc: 0.9865 - val_pred2_mean_pred: 0.4495 - val_pred3_acc: 0.9841 - val_pred3_mean_pred: 0.4610\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.19277 to 0.16974, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 9/100\n",
      "17500/17500 [==============================] - 7s 388us/step - loss: 0.1606 - pred1_loss: 0.0735 - pred2_loss: 0.0413 - pred3_loss: 0.0459 - pred1_acc: 0.9744 - pred1_mean_pred: 0.7349 - pred2_acc: 0.9870 - pred2_mean_pred: 0.4126 - pred3_acc: 0.9839 - pred3_mean_pred: 0.4587 - val_loss: 0.1538 - val_pred1_loss: 0.0672 - val_pred2_loss: 0.0417 - val_pred3_loss: 0.0449 - val_pred1_acc: 0.9780 - val_pred1_mean_pred: 0.6724 - val_pred2_acc: 0.9866 - val_pred2_mean_pred: 0.4168 - val_pred3_acc: 0.9844 - val_pred3_mean_pred: 0.4486\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.16974 to 0.15379, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 10/100\n",
      "17500/17500 [==============================] - 7s 393us/step - loss: 0.1471 - pred1_loss: 0.0671 - pred2_loss: 0.0377 - pred3_loss: 0.0424 - pred1_acc: 0.9769 - pred1_mean_pred: 0.6709 - pred2_acc: 0.9880 - pred2_mean_pred: 0.3767 - pred3_acc: 0.9851 - pred3_mean_pred: 0.4235 - val_loss: 0.1408 - val_pred1_loss: 0.0613 - val_pred2_loss: 0.0384 - val_pred3_loss: 0.0411 - val_pred1_acc: 0.9800 - val_pred1_mean_pred: 0.6125 - val_pred2_acc: 0.9880 - val_pred2_mean_pred: 0.3844 - val_pred3_acc: 0.9857 - val_pred3_mean_pred: 0.4108\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15379 to 0.14077, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 11/100\n",
      "17500/17500 [==============================] - 7s 395us/step - loss: 0.1364 - pred1_loss: 0.0615 - pred2_loss: 0.0352 - pred3_loss: 0.0397 - pred1_acc: 0.9792 - pred1_mean_pred: 0.6153 - pred2_acc: 0.9887 - pred2_mean_pred: 0.3522 - pred3_acc: 0.9861 - pred3_mean_pred: 0.3967 - val_loss: 0.1278 - val_pred1_loss: 0.0568 - val_pred2_loss: 0.0329 - val_pred3_loss: 0.0381 - val_pred1_acc: 0.9810 - val_pred1_mean_pred: 0.5683 - val_pred2_acc: 0.9888 - val_pred2_mean_pred: 0.3295 - val_pred3_acc: 0.9865 - val_pred3_mean_pred: 0.3806\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.14077 to 0.12784, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 12/100\n",
      "17500/17500 [==============================] - 7s 390us/step - loss: 0.1279 - pred1_loss: 0.0578 - pred2_loss: 0.0326 - pred3_loss: 0.0374 - pred1_acc: 0.9800 - pred1_mean_pred: 0.5780 - pred2_acc: 0.9895 - pred2_mean_pred: 0.3265 - pred3_acc: 0.9869 - pred3_mean_pred: 0.3742 - val_loss: 0.1284 - val_pred1_loss: 0.0562 - val_pred2_loss: 0.0336 - val_pred3_loss: 0.0386 - val_pred1_acc: 0.9818 - val_pred1_mean_pred: 0.5619 - val_pred2_acc: 0.9889 - val_pred2_mean_pred: 0.3362 - val_pred3_acc: 0.9871 - val_pred3_mean_pred: 0.3859\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.12784\n",
      "Epoch 13/100\n",
      "17500/17500 [==============================] - 7s 391us/step - loss: 0.1192 - pred1_loss: 0.0536 - pred2_loss: 0.0305 - pred3_loss: 0.0351 - pred1_acc: 0.9815 - pred1_mean_pred: 0.5360 - pred2_acc: 0.9901 - pred2_mean_pred: 0.3046 - pred3_acc: 0.9877 - pred3_mean_pred: 0.3511 - val_loss: 0.1229 - val_pred1_loss: 0.0538 - val_pred2_loss: 0.0318 - val_pred3_loss: 0.0373 - val_pred1_acc: 0.9827 - val_pred1_mean_pred: 0.5378 - val_pred2_acc: 0.9895 - val_pred2_mean_pred: 0.3181 - val_pred3_acc: 0.9871 - val_pred3_mean_pred: 0.3728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_loss improved from 0.12784 to 0.12286, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 14/100\n",
      "17500/17500 [==============================] - 7s 390us/step - loss: 0.1116 - pred1_loss: 0.0505 - pred2_loss: 0.0280 - pred3_loss: 0.0330 - pred1_acc: 0.9824 - pred1_mean_pred: 0.5053 - pred2_acc: 0.9909 - pred2_mean_pred: 0.2803 - pred3_acc: 0.9884 - pred3_mean_pred: 0.3305 - val_loss: 0.1260 - val_pred1_loss: 0.0510 - val_pred2_loss: 0.0361 - val_pred3_loss: 0.0388 - val_pred1_acc: 0.9832 - val_pred1_mean_pred: 0.5101 - val_pred2_acc: 0.9881 - val_pred2_mean_pred: 0.3610 - val_pred3_acc: 0.9870 - val_pred3_mean_pred: 0.3885\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12286\n",
      "Epoch 15/100\n",
      "17500/17500 [==============================] - 7s 394us/step - loss: 0.1069 - pred1_loss: 0.0479 - pred2_loss: 0.0272 - pred3_loss: 0.0318 - pred1_acc: 0.9833 - pred1_mean_pred: 0.4794 - pred2_acc: 0.9913 - pred2_mean_pred: 0.2718 - pred3_acc: 0.9888 - pred3_mean_pred: 0.3176 - val_loss: 0.1092 - val_pred1_loss: 0.0472 - val_pred2_loss: 0.0279 - val_pred3_loss: 0.0342 - val_pred1_acc: 0.9837 - val_pred1_mean_pred: 0.4717 - val_pred2_acc: 0.9912 - val_pred2_mean_pred: 0.2786 - val_pred3_acc: 0.9882 - val_pred3_mean_pred: 0.3420\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.12286 to 0.10923, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 16/100\n",
      "17500/17500 [==============================] - 7s 398us/step - loss: 0.1016 - pred1_loss: 0.0460 - pred2_loss: 0.0255 - pred3_loss: 0.0302 - pred1_acc: 0.9839 - pred1_mean_pred: 0.4597 - pred2_acc: 0.9917 - pred2_mean_pred: 0.2545 - pred3_acc: 0.9894 - pred3_mean_pred: 0.3017 - val_loss: 0.1108 - val_pred1_loss: 0.0478 - val_pred2_loss: 0.0277 - val_pred3_loss: 0.0352 - val_pred1_acc: 0.9835 - val_pred1_mean_pred: 0.4780 - val_pred2_acc: 0.9907 - val_pred2_mean_pred: 0.2773 - val_pred3_acc: 0.9887 - val_pred3_mean_pred: 0.3524\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10923\n",
      "Epoch 17/100\n",
      "17500/17500 [==============================] - 7s 399us/step - loss: 0.0966 - pred1_loss: 0.0438 - pred2_loss: 0.0240 - pred3_loss: 0.0288 - pred1_acc: 0.9850 - pred1_mean_pred: 0.4378 - pred2_acc: 0.9922 - pred2_mean_pred: 0.2401 - pred3_acc: 0.9900 - pred3_mean_pred: 0.2877 - val_loss: 0.1122 - val_pred1_loss: 0.0460 - val_pred2_loss: 0.0304 - val_pred3_loss: 0.0358 - val_pred1_acc: 0.9841 - val_pred1_mean_pred: 0.4599 - val_pred2_acc: 0.9902 - val_pred2_mean_pred: 0.3043 - val_pred3_acc: 0.9879 - val_pred3_mean_pred: 0.3579\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10923\n",
      "Fold 1\n",
      "Build model...\n",
      "Train...\n",
      "Train on 17500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a13381ef296c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheck_point\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m               validation_data=(train_seq[val_idx],{'pred1':train_labels1[val_idx],'pred2':train_labels2[val_idx],'pred3':train_labels[val_idx]}))\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model_weight/%s_%d.h5'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f4801404e6e1>\u001b[0m in \u001b[0;36mF\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \"\"\"\n\u001b[1;32m   1488\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1444\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1446\u001b[0;31m             session._session, options_ptr)\n\u001b[0m\u001b[1;32m   1447\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=100\n",
    "weight_name='V2'\n",
    "oof=np.zeros((len(train),29))\n",
    "tmp=0\n",
    "test_oof=np.zeros((len(test),29))\n",
    "\n",
    "folds=StratifiedKFold(n_splits=10,shuffle=True, random_state=2018) #2018\n",
    "for fold_n, (trn_idx, val_idx) in enumerate(folds.split(train,cate_num)):\n",
    "    print('Fold', fold_n)\n",
    "    print('Build model...')\n",
    "#     print('正样本比例:',train_labels[trn_idx].mean(0))\n",
    "    model=NN_huaweiv1(maxlen=128,embedding_matrix=embedding_matrix)\n",
    "#     model=multi_gpu_model(model,gpus=2)\n",
    "    model.compile('adam', ['binary_crossentropy','binary_crossentropy','binary_crossentropy'], metrics=['accuracy',mean_pred])\n",
    "    lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "    lookahead.inject(model) # add into model\n",
    "    print('Train...')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2,mode='min')\n",
    "    check_point=ModelCheckpoint('../model_weight/%s_%d.h5'%(weight_name,fold_n),monitor='val_loss',verbose=1, save_best_only=True,save_weights_only=True)\n",
    "\n",
    "    model.fit(train_seq[trn_idx],{'pred1':train_labels1[trn_idx],'pred2':train_labels2[trn_idx],'pred3':train_labels[trn_idx]},\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping,check_point],\n",
    "              validation_data=(train_seq[val_idx],{'pred1':train_labels1[val_idx],'pred2':train_labels2[val_idx],'pred3':train_labels[val_idx]}))\n",
    "    model.load_weights('../model_weight/%s_%d.h5'%(weight_name,fold_n))\n",
    "    _,_,oof[val_idx,:]= model.predict(train_seq[val_idx],batch_size=batch_size)\n",
    "    _,_,tmp_test_pred=model.predict(test_seq,batch_size=batch_size)\n",
    "    test_oof[:]=tmp_test_pred/folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "false-humanitarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:03:15.910141Z",
     "start_time": "2021-04-15T07:03:15.802681Z"
    }
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['report_ID']=test[0]\n",
    "sub['Prediction']=[ '|'+' '.join(['%.12f'%j for j in i]) for i in test_oof ]\n",
    "sub.to_csv('../result.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-merchant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
