{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unable-israel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T03:31:10.966776Z",
     "start_time": "2021-04-21T03:31:08.765009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  tensorflow.compat.v1 import keras\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "from tensorflow.compat.v1.keras import Model\n",
    "from tensorflow.compat.v1.keras import initializers, regularizers, constraints\n",
    "from tensorflow.compat.v1.keras.layers import Layer\n",
    "from tensorflow.compat.v1.keras.layers import Embedding, Dense, CuDNNLSTM,CuDNNGRU, Bidirectional,SpatialDropout1D,Input,GlobalAveragePooling1D,GlobalMaxPooling1D,Conv1D,concatenate,Dropout,Activation,BatchNormalization,Concatenate,Add,MaxPooling1D,Flatten,AveragePooling1D\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.compat.v1.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.compat.v1.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.compat.v1.keras.preprocessing import text, sequence\n",
    "import gensim, logging\n",
    "from tensorflow.compat.v1.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.model_selection import KFold,StratifiedShuffleSplit,StratifiedKFold\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pregnant-ozone",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T03:32:28.097875Z",
     "start_time": "2021-04-21T03:32:27.995171Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../tcdata/train.csv',header=None)\n",
    "test=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "# test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "data=pd.concat([train,test],axis=0)\n",
    "data[1]=data[1].apply(lambda x:x.strip().replace('|',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noble-manhattan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T03:32:28.917332Z",
     "start_time": "2021-04-21T03:32:28.817602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noncase label1: 0\n",
      "noncase label2: 8247\n"
     ]
    }
   ],
   "source": [
    "## 制作标签\n",
    "### 创建训练集标签 \n",
    "train_labels1=np.zeros((len(train),17),dtype='int8')\n",
    "noncase=0\n",
    "for cnt,i in enumerate(train[2]):\n",
    "    if i:\n",
    "        lab=[int(x.replace('|','').strip()) for x in i.split(' ') if x and x!='|']\n",
    "        for l in lab:\n",
    "            train_labels1[cnt,l]=1\n",
    "    else:\n",
    "        noncase+=1\n",
    "print('noncase label1:',noncase)\n",
    "#----------------------------------\n",
    "noncase=0\n",
    "train_labels2=np.zeros((len(train),12),dtype='int8')\n",
    "for cnt,i in enumerate(train[3]):\n",
    "    if pd.notna(i):\n",
    "        lab=[int(x.replace('|','').strip()) for x in i.split(' ') if x and x!='|']\n",
    "        for l in lab:\n",
    "            train_labels2[cnt,l]=1\n",
    "    else:\n",
    "        noncase+=1\n",
    "print('noncase label2:',noncase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "specified-hebrew",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T03:32:32.024488Z",
     "start_time": "2021-04-21T03:32:32.017126Z"
    }
   },
   "outputs": [],
   "source": [
    "cate_num=train_labels1.sum(1)+train_labels2.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "english-burlington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:02:19.297605Z",
     "start_time": "2021-04-15T06:02:19.227226Z"
    }
   },
   "outputs": [],
   "source": [
    "## 生成全部的训练文本\n",
    "data1=pd.read_csv('../tcdata/track1_round1_train_20210222.csv',header=None)\n",
    "data2=pd.read_csv('../tcdata/track1_round1_testA_20210222.csv',header=None)\n",
    "data3=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "text_data=pd.concat([train,data1,data2,data3],axis=0)\n",
    "text_data[1]=text_data[1].apply(lambda x:x.strip().replace('|',''))\n",
    "text_data[1].to_csv('../tmp/all_data.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "overhead-plenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:07:16.368462Z",
     "start_time": "2021-04-15T06:07:16.288461Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data=pd.read_csv('../tmp/all_data.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "certified-afternoon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T03:32:41.149995Z",
     "start_time": "2021-04-21T03:32:41.146574Z"
    }
   },
   "outputs": [],
   "source": [
    "# w2v=Word2Vec(text_data[0].apply(lambda x:x.split(' ')).tolist(),size=128, window=8, iter=30, min_count=2,\n",
    "#                      sg=1, sample=0.002, workers=6 , seed=1017)\n",
    "\n",
    "# w2v.wv.save_word2vec_format('../tmp/w2v_128.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dental-thing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T03:32:43.083095Z",
     "start_time": "2021-04-21T03:32:41.922195Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False, char_level=False, split=' ')\n",
    "tokenizer.fit_on_texts(data[1].tolist())\n",
    "seq = tokenizer.texts_to_sequences(data[1].tolist())\n",
    "# 分训练和测试集合\n",
    "seq = pad_sequences(seq, maxlen=128, value=0)\n",
    "train_seq=np.asarray(seq[:len(train)])\n",
    "test_seq=seq[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "median-nickname",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:00:10.178841Z",
     "start_time": "2021-04-15T07:00:10.172784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 128), dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "governmental-netscape",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T03:32:51.192505Z",
     "start_time": "2021-04-21T03:32:51.026806Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-21 11:32:51,033 : INFO : loading projection weights from ../tmp/w2v_128.txt\n",
      "2021-04-21 11:32:51,174 : INFO : loaded (859, 128) matrix from ../tmp/w2v_128.txt\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(859, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix=np.zeros((len(tokenizer.word_index)+1,128))\n",
    "w2v=gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        '../tmp/w2v_128.txt', binary=False)\n",
    "\n",
    "for word in tokenizer.word_index:\n",
    "    if word not in w2v.wv.vocab:\n",
    "        continue\n",
    "    embedding_matrix[tokenizer.word_index[word]] = w2v[word]\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "compact-bearing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:56:56.080887Z",
     "start_time": "2021-04-15T06:56:55.888120Z"
    }
   },
   "outputs": [],
   "source": [
    "def NN_huaweiv1(maxlen,embedding_matrix=None,class_num1=17,class_num2=12):    \n",
    "    emb_layer = Embedding(\n",
    "       embedding_matrix.shape[0], embedding_matrix.shape[1],input_length=maxlen,weights=[embedding_matrix],trainable=False,\n",
    "    )\n",
    "    seq1 = Input(shape=(maxlen,)) \n",
    "    \n",
    "    x1 = emb_layer(seq1)\n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "    lstm_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    gru_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    cnn1d_layer=Conv1D(64, kernel_size=3, padding=\"same\", kernel_initializer=\"he_uniform\")\n",
    "    x1 = sdrop(x1)\n",
    "    lstm1 = lstm_layer(x1)\n",
    "    gru1 = gru_layer(lstm1)\n",
    "    att_1 = Attention(maxlen)(lstm1)\n",
    "    att_2 = Attention(maxlen)(gru1)\n",
    "    cnn1 = cnn1d_layer(lstm1)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()\n",
    "    max_pool = GlobalMaxPooling1D()\n",
    "\n",
    "    x1=concatenate([att_1,att_2,Attention(maxlen)(cnn1),avg_pool(cnn1),max_pool(cnn1)])\n",
    "\n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(128)(x1))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(64)(x)))\n",
    "    pred1 = Dense(class_num1, activation='sigmoid',name='pred1')(x)\n",
    "    y=concatenate([x1,x])\n",
    "    y = Activation(activation=\"relu\")(BatchNormalization()(Dense(64)(x)))\n",
    "    pred2=Dense(class_num2, activation='sigmoid',name='pred2')(y)\n",
    "    \n",
    "    model = Model(inputs=seq1, outputs=[pred1,pred2])\n",
    "    return model\n",
    "\n",
    "class AttentionSelf(Layer):\n",
    "    \"\"\"\n",
    "        self attention,\n",
    "        codes from:  https://mp.weixin.qq.com/s/qmJnyFMkXVjYBwoR_AQLVA\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # W、K and V\n",
    "        self.kernel = self.add_weight(name='WKV',\n",
    "                                        shape=(3, input_shape[2], self.output_dim),\n",
    "                                        initializer='uniform',\n",
    "                                        regularizer=L1L2(0.0000032),\n",
    "                                        trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    "        # print(\"WQ.shape\",WQ.shape)\n",
    "        # print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\n",
    "        QK = K.batch_dot(WQ, K.permute_dimensions(WK, [0, 2, 1]))\n",
    "        QK = QK / (64**0.5)\n",
    "        QK = K.softmax(QK)\n",
    "        # print(\"QK.shape\",QK.shape)\n",
    "        V = K.batch_dot(QK, WV)\n",
    "        return V\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], self.output_dim)\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "#         print('-------------',type(input_shape))\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return -K.mean(y_true*K.log(y_pred+1.e-7)+(1-y_true)*K.log(1-y_pred+1.e-7))*10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-amsterdam",
   "metadata": {},
   "source": [
    "## HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANGraph():\n",
    "    def __init__(self, hyper_parameters):\n",
    "        \"\"\"\n",
    "            初始化\n",
    "        :param hyper_parameters: json，超参\n",
    "        \"\"\"\n",
    "        self.rnn_type = hyper_parameters['model'].get('rnn_type', 'Bidirectional-LSTM')\n",
    "        self.rnn_units = hyper_parameters['model'].get('rnn_units', 256)\n",
    "        self.attention_units = hyper_parameters['model'].get('attention_units', self.rnn_units*2)\n",
    "        self.dropout_spatial = hyper_parameters['model'].get('droupout_spatial', 0.2)\n",
    "        self.len_max_sen = hyper_parameters['model'].get('len_max_sen', 50)\n",
    "        super().__init__(hyper_parameters)\n",
    "\n",
    "    def create_model(self, hyper_parameters):\n",
    "        \"\"\"\n",
    "            构建神经网络\n",
    "        :param hyper_parameters:json,  hyper parameters of network\n",
    "        :return: tensor, moedl\n",
    "        \"\"\"\n",
    "        super().create_model(hyper_parameters)\n",
    "        # char or word\n",
    "        x_input_word = self.word_embedding.output\n",
    "        x_word = self.word_level()(x_input_word)\n",
    "        x_word_to_sen = Dropout(self.dropout)(x_word)\n",
    "\n",
    "        # sentence or doc\n",
    "        x_sen = self.sentence_level()(x_word_to_sen)\n",
    "        x_sen = Dropout(self.dropout)(x_sen)\n",
    "\n",
    "        x_sen = Flatten()(x_sen)\n",
    "        # 最后就是softmax\n",
    "        dense_layer = Dense(self.label, activation=self.activate_classify)(x_sen)\n",
    "        output = [dense_layer]\n",
    "        self.model = Model(self.word_embedding.input, output)\n",
    "        self.model.summary(132)\n",
    "\n",
    "    def word_level(self):\n",
    "        x_input_word = Input(shape=(self.len_max, self.embed_size))\n",
    "        # x = SpatialDropout1D(self.dropout_spatial)(x_input_word)\n",
    "        x = Bidirectional(GRU(units=self.rnn_units,\n",
    "                              return_sequences=True,\n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=regularizers.l2(self.l2),\n",
    "                              recurrent_regularizer=regularizers.l2(self.l2)))(x_input_word)\n",
    "        out_sent = AttentionSelf(self.rnn_units*2)(x)\n",
    "        model = Model(x_input_word, out_sent)\n",
    "        return model\n",
    "\n",
    "    def sentence_level(self):\n",
    "        x_input_sen = Input(shape=(self.len_max, self.rnn_units*2))\n",
    "        # x = SpatialDropout1D(self.dropout_spatial)(x_input_sen)\n",
    "        output_doc = Bidirectional(GRU(units=self.rnn_units*2,\n",
    "                              return_sequences=True,\n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=regularizers.l2(self.l2),\n",
    "                              recurrent_regularizer=regularizers.l2(self.l2)))(x_input_sen)\n",
    "        output_doc_att = AttentionSelf(self.word_embedding.embed_size)(output_doc)\n",
    "        model = Model(x_input_sen, output_doc_att)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "accurate-butler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:58:57.673673Z",
     "start_time": "2021-04-15T06:56:57.036978Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=8.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Build model...\n",
      "Train...\n",
      "Train on 17500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      "17500/17500 [==============================] - 14s 806us/step - loss: 0.6531 - pred1_loss: 0.3460 - pred2_loss: 0.3071 - pred1_acc: 0.8706 - pred1_mean_pred: 3.4595 - pred2_acc: 0.8911 - pred2_mean_pred: 3.0715 - val_loss: 0.3001 - val_pred1_loss: 0.1798 - val_pred2_loss: 0.1203 - val_pred1_acc: 0.9444 - val_pred1_mean_pred: 1.7977 - val_pred2_acc: 0.9663 - val_pred2_mean_pred: 1.2028\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30005, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 2/100\n",
      "17500/17500 [==============================] - 6s 341us/step - loss: 0.2520 - pred1_loss: 0.1536 - pred2_loss: 0.0984 - pred1_acc: 0.9483 - pred1_mean_pred: 1.5355 - pred2_acc: 0.9724 - pred2_mean_pred: 0.9843 - val_loss: 0.2137 - val_pred1_loss: 0.1333 - val_pred2_loss: 0.0803 - val_pred1_acc: 0.9539 - val_pred1_mean_pred: 1.3333 - val_pred2_acc: 0.9760 - val_pred2_mean_pred: 0.8035\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.30005 to 0.21368, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 3/100\n",
      "17500/17500 [==============================] - 6s 336us/step - loss: 0.1926 - pred1_loss: 0.1210 - pred2_loss: 0.0716 - pred1_acc: 0.9549 - pred1_mean_pred: 1.2104 - pred2_acc: 0.9779 - pred2_mean_pred: 0.7155 - val_loss: 0.1753 - val_pred1_loss: 0.1086 - val_pred2_loss: 0.0667 - val_pred1_acc: 0.9572 - val_pred1_mean_pred: 1.0856 - val_pred2_acc: 0.9796 - val_pred2_mean_pred: 0.6673\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.21368 to 0.17529, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 4/100\n",
      "17500/17500 [==============================] - 6s 333us/step - loss: 0.1627 - pred1_loss: 0.1049 - pred2_loss: 0.0577 - pred1_acc: 0.9579 - pred1_mean_pred: 1.0494 - pred2_acc: 0.9817 - pred2_mean_pred: 0.5773 - val_loss: 0.1515 - val_pred1_loss: 0.0957 - val_pred2_loss: 0.0558 - val_pred1_acc: 0.9591 - val_pred1_mean_pred: 0.9569 - val_pred2_acc: 0.9815 - val_pred2_mean_pred: 0.5581\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17529 to 0.15150, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 5/100\n",
      "17500/17500 [==============================] - 6s 331us/step - loss: 0.1430 - pred1_loss: 0.0948 - pred2_loss: 0.0482 - pred1_acc: 0.9603 - pred1_mean_pred: 0.9476 - pred2_acc: 0.9844 - pred2_mean_pred: 0.4821 - val_loss: 0.1328 - val_pred1_loss: 0.0881 - val_pred2_loss: 0.0447 - val_pred1_acc: 0.9626 - val_pred1_mean_pred: 0.8809 - val_pred2_acc: 0.9854 - val_pred2_mean_pred: 0.4472\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15150 to 0.13281, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 6/100\n",
      "17500/17500 [==============================] - 6s 331us/step - loss: 0.1262 - pred1_loss: 0.0837 - pred2_loss: 0.0425 - pred1_acc: 0.9658 - pred1_mean_pred: 0.8371 - pred2_acc: 0.9860 - pred2_mean_pred: 0.4253 - val_loss: 0.1137 - val_pred1_loss: 0.0706 - val_pred2_loss: 0.0431 - val_pred1_acc: 0.9751 - val_pred1_mean_pred: 0.7065 - val_pred2_acc: 0.9865 - val_pred2_mean_pred: 0.4305\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13281 to 0.11370, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 7/100\n",
      "17500/17500 [==============================] - 6s 338us/step - loss: 0.1067 - pred1_loss: 0.0681 - pred2_loss: 0.0385 - pred1_acc: 0.9754 - pred1_mean_pred: 0.6811 - pred2_acc: 0.9873 - pred2_mean_pred: 0.3855 - val_loss: 0.0956 - val_pred1_loss: 0.0579 - val_pred2_loss: 0.0377 - val_pred1_acc: 0.9810 - val_pred1_mean_pred: 0.5794 - val_pred2_acc: 0.9879 - val_pred2_mean_pred: 0.3766\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.11370 to 0.09560, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 8/100\n",
      "17500/17500 [==============================] - 6s 336us/step - loss: 0.0934 - pred1_loss: 0.0586 - pred2_loss: 0.0348 - pred1_acc: 0.9795 - pred1_mean_pred: 0.5861 - pred2_acc: 0.9884 - pred2_mean_pred: 0.3479 - val_loss: 0.0873 - val_pred1_loss: 0.0534 - val_pred2_loss: 0.0340 - val_pred1_acc: 0.9813 - val_pred1_mean_pred: 0.5335 - val_pred2_acc: 0.9886 - val_pred2_mean_pred: 0.3395\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09560 to 0.08731, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 9/100\n",
      "17500/17500 [==============================] - 6s 336us/step - loss: 0.0842 - pred1_loss: 0.0532 - pred2_loss: 0.0310 - pred1_acc: 0.9815 - pred1_mean_pred: 0.5322 - pred2_acc: 0.9899 - pred2_mean_pred: 0.3098 - val_loss: 0.0874 - val_pred1_loss: 0.0522 - val_pred2_loss: 0.0352 - val_pred1_acc: 0.9819 - val_pred1_mean_pred: 0.5222 - val_pred2_acc: 0.9884 - val_pred2_mean_pred: 0.3521\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08731\n",
      "Epoch 10/100\n",
      "17500/17500 [==============================] - 6s 326us/step - loss: 0.0770 - pred1_loss: 0.0488 - pred2_loss: 0.0282 - pred1_acc: 0.9829 - pred1_mean_pred: 0.4875 - pred2_acc: 0.9908 - pred2_mean_pred: 0.2821 - val_loss: 0.0806 - val_pred1_loss: 0.0499 - val_pred2_loss: 0.0307 - val_pred1_acc: 0.9834 - val_pred1_mean_pred: 0.4990 - val_pred2_acc: 0.9893 - val_pred2_mean_pred: 0.3066\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08731 to 0.08056, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 11/100\n",
      "17500/17500 [==============================] - 6s 331us/step - loss: 0.0703 - pred1_loss: 0.0445 - pred2_loss: 0.0258 - pred1_acc: 0.9842 - pred1_mean_pred: 0.4451 - pred2_acc: 0.9913 - pred2_mean_pred: 0.2581 - val_loss: 0.0715 - val_pred1_loss: 0.0435 - val_pred2_loss: 0.0280 - val_pred1_acc: 0.9854 - val_pred1_mean_pred: 0.4347 - val_pred2_acc: 0.9902 - val_pred2_mean_pred: 0.2802\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08056 to 0.07149, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 12/100\n",
      "17500/17500 [==============================] - 6s 335us/step - loss: 0.0652 - pred1_loss: 0.0416 - pred2_loss: 0.0235 - pred1_acc: 0.9856 - pred1_mean_pred: 0.4163 - pred2_acc: 0.9923 - pred2_mean_pred: 0.2353 - val_loss: 0.0675 - val_pred1_loss: 0.0404 - val_pred2_loss: 0.0271 - val_pred1_acc: 0.9865 - val_pred1_mean_pred: 0.4036 - val_pred2_acc: 0.9913 - val_pred2_mean_pred: 0.2714\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.07149 to 0.06749, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 13/100\n",
      "17500/17500 [==============================] - 6s 331us/step - loss: 0.0605 - pred1_loss: 0.0386 - pred2_loss: 0.0218 - pred1_acc: 0.9867 - pred1_mean_pred: 0.3863 - pred2_acc: 0.9926 - pred2_mean_pred: 0.2184 - val_loss: 0.0669 - val_pred1_loss: 0.0392 - val_pred2_loss: 0.0277 - val_pred1_acc: 0.9868 - val_pred1_mean_pred: 0.3918 - val_pred2_acc: 0.9911 - val_pred2_mean_pred: 0.2774\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06749 to 0.06692, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 14/100\n",
      "17500/17500 [==============================] - 6s 334us/step - loss: 0.0562 - pred1_loss: 0.0367 - pred2_loss: 0.0195 - pred1_acc: 0.9870 - pred1_mean_pred: 0.3671 - pred2_acc: 0.9936 - pred2_mean_pred: 0.1953 - val_loss: 0.0650 - val_pred1_loss: 0.0396 - val_pred2_loss: 0.0254 - val_pred1_acc: 0.9863 - val_pred1_mean_pred: 0.3965 - val_pred2_acc: 0.9919 - val_pred2_mean_pred: 0.2538\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.06692 to 0.06503, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 15/100\n",
      "17500/17500 [==============================] - 6s 333us/step - loss: 0.0545 - pred1_loss: 0.0352 - pred2_loss: 0.0193 - pred1_acc: 0.9876 - pred1_mean_pred: 0.3519 - pred2_acc: 0.9935 - pred2_mean_pred: 0.1927 - val_loss: 0.0672 - val_pred1_loss: 0.0409 - val_pred2_loss: 0.0263 - val_pred1_acc: 0.9858 - val_pred1_mean_pred: 0.4087 - val_pred2_acc: 0.9923 - val_pred2_mean_pred: 0.2635\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06503\n",
      "Epoch 16/100\n",
      "17500/17500 [==============================] - 6s 335us/step - loss: 0.0497 - pred1_loss: 0.0327 - pred2_loss: 0.0170 - pred1_acc: 0.9882 - pred1_mean_pred: 0.3275 - pred2_acc: 0.9943 - pred2_mean_pred: 0.1700 - val_loss: 0.0606 - val_pred1_loss: 0.0369 - val_pred2_loss: 0.0238 - val_pred1_acc: 0.9872 - val_pred1_mean_pred: 0.3688 - val_pred2_acc: 0.9925 - val_pred2_mean_pred: 0.2375\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.06503 to 0.06063, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 17/100\n",
      "17500/17500 [==============================] - 6s 331us/step - loss: 0.0470 - pred1_loss: 0.0309 - pred2_loss: 0.0161 - pred1_acc: 0.9891 - pred1_mean_pred: 0.3090 - pred2_acc: 0.9946 - pred2_mean_pred: 0.1612 - val_loss: 0.0663 - val_pred1_loss: 0.0382 - val_pred2_loss: 0.0281 - val_pred1_acc: 0.9868 - val_pred1_mean_pred: 0.3825 - val_pred2_acc: 0.9917 - val_pred2_mean_pred: 0.2808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06063\n",
      "Epoch 18/100\n",
      "17500/17500 [==============================] - 6s 337us/step - loss: 0.0441 - pred1_loss: 0.0297 - pred2_loss: 0.0145 - pred1_acc: 0.9894 - pred1_mean_pred: 0.2967 - pred2_acc: 0.9951 - pred2_mean_pred: 0.1447 - val_loss: 0.0612 - val_pred1_loss: 0.0371 - val_pred2_loss: 0.0241 - val_pred1_acc: 0.9875 - val_pred1_mean_pred: 0.3708 - val_pred2_acc: 0.9924 - val_pred2_mean_pred: 0.2412\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06063\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-fc7f8113f48b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model_weight/%s_%d.h5'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtmp_test_pred1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp_test_pred2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtest_oof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtmp_test_pred1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtest_oof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtmp_test_pred2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=100\n",
    "weight_name='V2'\n",
    "oof=np.zeros((len(train),29))\n",
    "tmp=0\n",
    "test_oof=np.zeros((len(test),29))\n",
    "# for ii in range(17):\n",
    "#     per_labels=train_labels[:,ii]\n",
    "#     print('当前分类类别：'+str(ii))\n",
    "#     print('正样本比例:'+str(sum(per_labels)/len(per_labels)))\n",
    "# per_labels=train_labels[:,1]\n",
    "folds=StratifiedKFold(n_splits=8,shuffle=True, random_state=2018) #2018\n",
    "for fold_n, (trn_idx, val_idx) in enumerate(folds.split(train,cate_num)):\n",
    "    print('Fold', fold_n)\n",
    "    print('Build model...')\n",
    "#     print('正样本比例:',train_labels[trn_idx].mean(0))\n",
    "    model=NN_huaweiv1(maxlen=128,embedding_matrix=embedding_matrix)\n",
    "#     model=multi_gpu_model(model,gpus=2)\n",
    "    model.compile('adam', ['binary_crossentropy','binary_crossentropy'], metrics=['accuracy',mean_pred])\n",
    "\n",
    "    print('Train...')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2,mode='min')\n",
    "    check_point=ModelCheckpoint('../model_weight/%s_%d.h5'%(weight_name,fold_n),monitor='val_loss',verbose=1, save_best_only=True,save_weights_only=True)\n",
    "\n",
    "    model.fit(train_seq[trn_idx],{'pred1':train_labels1[trn_idx],'pred2':train_labels2[trn_idx]},\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping,check_point],\n",
    "              validation_data=(train_seq[val_idx],{'pred1':train_labels1[val_idx],'pred2':train_labels2[val_idx]}))\n",
    "    model.load_weights('../model_weight/%s_%d.h5'%(weight_name,fold_n))\n",
    "    oof[val_idx,:17],oof[val_idx,17:] = model.predict(train_seq[val_idx],batch_size=batch_size)\n",
    "    tmp_test_pred1,tmp_test_pred2=model.predict(test_seq,batch_size=batch_size)\n",
    "    test_oof[:,:17]+=tmp_test_pred1/folds.n_splits\n",
    "    test_oof[:,17:]+=tmp_test_pred2/folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "false-humanitarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:03:15.910141Z",
     "start_time": "2021-04-15T07:03:15.802681Z"
    }
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['report_ID']=test[0]\n",
    "sub['Prediction']=[ '|'+' '.join(['%.12f'%j for j in i]) for i in test_oof ]\n",
    "sub.to_csv('../result.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-merchant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
