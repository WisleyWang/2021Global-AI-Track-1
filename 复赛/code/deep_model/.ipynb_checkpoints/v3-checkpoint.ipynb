{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras import initializers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "unable-israel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:45:52.559843Z",
     "start_time": "2021-04-16T07:45:52.546924Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import Model\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Embedding, Dense, CuDNNLSTM,CuDNNGRU, Bidirectional,SpatialDropout1D,Input,\\\n",
    "GlobalAveragePooling1D,GlobalMaxPooling1D,Conv1D,concatenate,Dropout,Activation,BatchNormalization,Concatenate,Add,\\\n",
    "MaxPooling1D,Flatten,AveragePooling1D\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "import gensim, logging\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.model_selection import KFold,StratifiedShuffleSplit,StratifiedKFold\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "pregnant-ozone",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:45:52.955237Z",
     "start_time": "2021-04-16T07:45:52.857307Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../tcdata/train.csv',header=None)\n",
    "test=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "# test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "data=pd.concat([train,test],axis=0)\n",
    "data[1]=data[1].apply(lambda x:x.strip().replace('|',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "noble-manhattan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:45:53.339825Z",
     "start_time": "2021-04-16T07:45:53.232328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noncase label1: 0\n",
      "noncase label2: 8247\n"
     ]
    }
   ],
   "source": [
    "## 制作标签\n",
    "### 创建训练集标签 \n",
    "train_labels1=np.zeros((len(train),17),dtype='int8')\n",
    "noncase=0\n",
    "for cnt,i in enumerate(train[2]):\n",
    "    if i:\n",
    "        lab=[int(x.replace('|','').strip()) for x in i.split(' ') if x and x!='|']\n",
    "        for l in lab:\n",
    "            train_labels1[cnt,l]=1\n",
    "    else:\n",
    "        noncase+=1\n",
    "print('noncase label1:',noncase)\n",
    "#----------------------------------\n",
    "noncase=0\n",
    "train_labels2=np.zeros((len(train),12),dtype='int8')\n",
    "for cnt,i in enumerate(train[3]):\n",
    "    if pd.notna(i):\n",
    "        lab=[int(x.replace('|','').strip()) for x in i.split(' ') if x and x!='|']\n",
    "        for l in lab:\n",
    "            train_labels2[cnt,l]=1\n",
    "    else:\n",
    "        noncase+=1\n",
    "print('noncase label2:',noncase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "specified-hebrew",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:45:54.785089Z",
     "start_time": "2021-04-16T07:45:54.777131Z"
    }
   },
   "outputs": [],
   "source": [
    "cate_num=train_labels1.sum(1)+train_labels2.sum(1)\n",
    "train_labels=np.concatenate([train_labels1,train_labels2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "english-burlington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:02:19.297605Z",
     "start_time": "2021-04-15T06:02:19.227226Z"
    }
   },
   "outputs": [],
   "source": [
    "## 生成全部的训练文本\n",
    "data1=pd.read_csv('../tcdata/track1_round1_train_20210222.csv',header=None)\n",
    "data2=pd.read_csv('../tcdata/track1_round1_testA_20210222.csv',header=None)\n",
    "data3=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "text_data=pd.concat([train,data1,data2,data3],axis=0)\n",
    "text_data[1]=text_data[1].apply(lambda x:x.strip().replace('|',''))\n",
    "text_data[1].to_csv('../tmp/all_data.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "overhead-plenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:07:16.368462Z",
     "start_time": "2021-04-15T06:07:16.288461Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data=pd.read_csv('../tmp/all_data.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "certified-afternoon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:58:19.724283Z",
     "start_time": "2021-04-16T03:58:19.720582Z"
    }
   },
   "outputs": [],
   "source": [
    "# w2v=Word2Vec(text_data[0].apply(lambda x:x.split(' ')).tolist(),size=128, window=8, iter=30, min_count=2,\n",
    "#                      sg=1, sample=0.002, workers=6 , seed=1017)\n",
    "\n",
    "# w2v.wv.save_word2vec_format('../tmp/w2v_128.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dental-thing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:45:59.233129Z",
     "start_time": "2021-04-16T07:45:58.001550Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False, char_level=False, split=' ')\n",
    "tokenizer.fit_on_texts(data[1].tolist())\n",
    "seq = tokenizer.texts_to_sequences(data[1].tolist())\n",
    "# 分训练和测试集合\n",
    "seq = pad_sequences(seq, maxlen=128, value=0)\n",
    "train_seq=np.asarray(seq[:len(train)])\n",
    "test_seq=seq[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "governmental-netscape",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:45:59.366773Z",
     "start_time": "2021-04-16T07:45:59.235551Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-16 15:45:59,238 : INFO : loading projection weights from ../tmp/w2v_128.txt\n",
      "2021-04-16 15:45:59,354 : INFO : loaded (859, 128) matrix from ../tmp/w2v_128.txt\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(859, 128)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix=np.zeros((len(tokenizer.word_index)+1,128))\n",
    "w2v=gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        '../tmp/w2v_128.txt', binary=False)\n",
    "\n",
    "for word in tokenizer.word_index:\n",
    "    if word not in w2v.wv.vocab:\n",
    "        continue\n",
    "    embedding_matrix[tokenizer.word_index[word]] = w2v[word]\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "spare-personal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:45:59.370725Z",
     "start_time": "2021-04-16T07:45:59.368307Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda,Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "compact-bearing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:45:59.639298Z",
     "start_time": "2021-04-16T07:45:59.616457Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "#         print('-------------',type(input_shape))\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "       \n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        c=a * x\n",
    "        c = K.sum(c, axis=1,keepdims=True)\n",
    "        return c\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape=list(input_shape)\n",
    "        output_shape[1]=1\n",
    "        return tuple(output_shape)\n",
    "def NN_huaweiv1(maxlen,embedding_matrix=None,class_num1=17,class_num2=12):    \n",
    "    emb_layer = Embedding(\n",
    "       embedding_matrix.shape[0], embedding_matrix.shape[1],input_length=maxlen,weights=[embedding_matrix],trainable=False,\n",
    "    )\n",
    "    seq1 = Input(shape=(maxlen,)) \n",
    "    \n",
    "    x1 = emb_layer(seq1)\n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "    lstm_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    gru_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    cnn1d_layer=Conv1D(64, kernel_size=3, padding=\"same\", kernel_initializer=\"he_uniform\")\n",
    "    x1 = sdrop(x1)\n",
    "    lstm1 = lstm_layer(x1)\n",
    "    gru1 = gru_layer(lstm1)\n",
    "    att_list=[]\n",
    "    for i in range(29):\n",
    "        att_list.append(Attention(maxlen)(gru1))\n",
    "    x=concatenate(att_list,axis=1)\n",
    "    x=Activation(activation=\"relu\")(BatchNormalization()(Dense(64)(x)))\n",
    "    x=Dense(1,activation='sigmoid')(x)\n",
    "    pred=Lambda(lambda x:x[:,:,0])(x)\n",
    "    model = Model(inputs=seq1, outputs=pred)\n",
    "    return model\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return -K.mean(y_true*K.log(y_pred+1.e-7)+(1-y_true)*K.log(1-y_pred+1.e-7))*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "accurate-butler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T07:54:40.348287Z",
     "start_time": "2021-04-16T07:49:35.953731Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=8.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Build model...\n",
      "(?, 1, 256)\n",
      "Train...\n",
      "Train on 17500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      "17500/17500 [==============================] - 19s 1ms/step - loss: 0.2463 - acc: 0.9221 - mean_pred: 2.4627 - val_loss: 0.1813 - val_acc: 0.9459 - val_mean_pred: 1.8127\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18127, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 2/100\n",
      "17500/17500 [==============================] - 10s 584us/step - loss: 0.1404 - acc: 0.9536 - mean_pred: 1.4038 - val_loss: 0.1558 - val_acc: 0.9551 - val_mean_pred: 1.5577\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18127 to 0.15577, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 3/100\n",
      "17500/17500 [==============================] - 10s 590us/step - loss: 0.1078 - acc: 0.9616 - mean_pred: 1.0783 - val_loss: 0.1431 - val_acc: 0.9604 - val_mean_pred: 1.4308\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15577 to 0.14308, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 4/100\n",
      "17500/17500 [==============================] - 10s 594us/step - loss: 0.0878 - acc: 0.9672 - mean_pred: 0.8778 - val_loss: 0.0991 - val_acc: 0.9666 - val_mean_pred: 0.9912\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14308 to 0.09912, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 5/100\n",
      "17500/17500 [==============================] - 10s 599us/step - loss: 0.0770 - acc: 0.9698 - mean_pred: 0.7696 - val_loss: 0.0794 - val_acc: 0.9680 - val_mean_pred: 0.7942\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09912 to 0.07942, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 6/100\n",
      "17500/17500 [==============================] - 11s 616us/step - loss: 0.0695 - acc: 0.9719 - mean_pred: 0.6948 - val_loss: 0.0805 - val_acc: 0.9693 - val_mean_pred: 0.8050\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07942\n",
      "Epoch 7/100\n",
      "17500/17500 [==============================] - 11s 625us/step - loss: 0.0640 - acc: 0.9733 - mean_pred: 0.6401 - val_loss: 0.0667 - val_acc: 0.9722 - val_mean_pred: 0.6672\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07942 to 0.06672, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 8/100\n",
      "17500/17500 [==============================] - 11s 631us/step - loss: 0.0577 - acc: 0.9766 - mean_pred: 0.5774 - val_loss: 0.0606 - val_acc: 0.9765 - val_mean_pred: 0.6059\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06672 to 0.06059, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 9/100\n",
      "17500/17500 [==============================] - 11s 635us/step - loss: 0.0482 - acc: 0.9822 - mean_pred: 0.4825 - val_loss: 0.0463 - val_acc: 0.9844 - val_mean_pred: 0.4628\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06059 to 0.04629, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 10/100\n",
      "17500/17500 [==============================] - 11s 638us/step - loss: 0.0415 - acc: 0.9854 - mean_pred: 0.4154 - val_loss: 0.0456 - val_acc: 0.9846 - val_mean_pred: 0.4560\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04629 to 0.04560, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 11/100\n",
      "17500/17500 [==============================] - 11s 638us/step - loss: 0.0373 - acc: 0.9868 - mean_pred: 0.3732 - val_loss: 0.0453 - val_acc: 0.9852 - val_mean_pred: 0.4526\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04560 to 0.04526, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 12/100\n",
      "17500/17500 [==============================] - 11s 643us/step - loss: 0.0338 - acc: 0.9882 - mean_pred: 0.3376 - val_loss: 0.0399 - val_acc: 0.9865 - val_mean_pred: 0.3993\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04526 to 0.03993, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 13/100\n",
      "17500/17500 [==============================] - 11s 649us/step - loss: 0.0312 - acc: 0.9890 - mean_pred: 0.3116 - val_loss: 0.0374 - val_acc: 0.9870 - val_mean_pred: 0.3739\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03993 to 0.03739, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 14/100\n",
      "17500/17500 [==============================] - 11s 642us/step - loss: 0.0292 - acc: 0.9899 - mean_pred: 0.2922 - val_loss: 0.0375 - val_acc: 0.9868 - val_mean_pred: 0.3754\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03739\n",
      "Epoch 15/100\n",
      "17500/17500 [==============================] - 11s 640us/step - loss: 0.0268 - acc: 0.9906 - mean_pred: 0.2684 - val_loss: 0.0385 - val_acc: 0.9876 - val_mean_pred: 0.3852\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03739\n",
      "Epoch 16/100\n",
      "17500/17500 [==============================] - 11s 641us/step - loss: 0.0258 - acc: 0.9910 - mean_pred: 0.2577 - val_loss: 0.0361 - val_acc: 0.9877 - val_mean_pred: 0.3610\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03739 to 0.03610, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 17/100\n",
      "17500/17500 [==============================] - 11s 645us/step - loss: 0.0231 - acc: 0.9918 - mean_pred: 0.2315 - val_loss: 0.0378 - val_acc: 0.9877 - val_mean_pred: 0.3784\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.03610\n",
      "Epoch 18/100\n",
      "17500/17500 [==============================] - 11s 646us/step - loss: 0.0213 - acc: 0.9923 - mean_pred: 0.2127 - val_loss: 0.0349 - val_acc: 0.9881 - val_mean_pred: 0.3490\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03610 to 0.03490, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 19/100\n",
      "17500/17500 [==============================] - 11s 645us/step - loss: 0.0203 - acc: 0.9928 - mean_pred: 0.2033 - val_loss: 0.0349 - val_acc: 0.9884 - val_mean_pred: 0.3488\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.03490 to 0.03488, saving model to ../model_weight/V3_0.h5\n",
      "Epoch 20/100\n",
      "17500/17500 [==============================] - 11s 645us/step - loss: 0.0184 - acc: 0.9935 - mean_pred: 0.1837 - val_loss: 0.0353 - val_acc: 0.9887 - val_mean_pred: 0.3526\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03488\n",
      "Epoch 21/100\n",
      "17500/17500 [==============================] - 12s 659us/step - loss: 0.0174 - acc: 0.9939 - mean_pred: 0.1736 - val_loss: 0.0374 - val_acc: 0.9890 - val_mean_pred: 0.3742\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03488\n",
      "Epoch 22/100\n",
      "17500/17500 [==============================] - 11s 657us/step - loss: 0.0163 - acc: 0.9941 - mean_pred: 0.1633 - val_loss: 0.0386 - val_acc: 0.9893 - val_mean_pred: 0.3863\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03488\n",
      "Fold 1\n",
      "Build model...\n",
      "(?, 1, 256)\n",
      "Train...\n",
      "Train on 17500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-15d366baa3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheck_point\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m               validation_data=(train_seq[val_idx],train_labels[val_idx]))\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model_weight/%s_%d.h5'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \"\"\"\n\u001b[1;32m   1488\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1444\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1446\u001b[0;31m             session._session, options_ptr)\n\u001b[0m\u001b[1;32m   1447\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=100\n",
    "weight_name='V3'\n",
    "oof=np.zeros((len(train),29))\n",
    "tmp=0\n",
    "test_oof=np.zeros((len(test),29))\n",
    "# for ii in range(17):\n",
    "#     per_labels=train_labels[:,ii]\n",
    "#     print('当前分类类别：'+str(ii))\n",
    "#     print('正样本比例:'+str(sum(per_labels)/len(per_labels)))\n",
    "# per_labels=train_labels[:,1]\n",
    "folds=StratifiedKFold(n_splits=8,shuffle=True, random_state=2018) #2018\n",
    "for fold_n, (trn_idx, val_idx) in enumerate(folds.split(train,cate_num)):\n",
    "    print('Fold', fold_n)\n",
    "    print('Build model...')\n",
    "#     print('正样本比例:',train_labels[trn_idx].mean(0))\n",
    "    model=NN_huaweiv1(128,embedding_matrix=embedding_matrix,class_num1=17,class_num2=12)\n",
    "#     model=multi_gpu_model(model,gpus=2)\n",
    "    model.compile('adam', ['binary_crossentropy'], metrics=['accuracy',mean_pred])\n",
    "\n",
    "    print('Train...')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3,mode='min')\n",
    "    check_point=ModelCheckpoint('../model_weight/%s_%d.h5'%(weight_name,fold_n),monitor='val_loss',verbose=1, save_best_only=True,save_weights_only=True)\n",
    "\n",
    "    model.fit(train_seq[trn_idx],train_labels[trn_idx],\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping,check_point],\n",
    "              validation_data=(train_seq[val_idx],train_labels[val_idx]))\n",
    "    model.load_weights('../model_weight/%s_%d.h5'%(weight_name,fold_n))\n",
    "    oof[val_idx,:] = model.predict(train_seq[val_idx],batch_size=batch_size)\n",
    "    test_oof[:,:]+=model.predict(test_seq,batch_size=batch_size)/folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "false-humanitarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:03:15.910141Z",
     "start_time": "2021-04-15T07:03:15.802681Z"
    }
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['report_ID']=test[0]\n",
    "sub['Prediction']=[ '|'+' '.join(['%.12f'%j for j in i]) for i in test_oof ]\n",
    "sub.to_csv('../result.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "extended-merchant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T08:27:51.609377Z",
     "start_time": "2021-04-16T08:27:51.601064Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "canadian-uruguay",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T08:27:52.621469Z",
     "start_time": "2021-04-16T08:27:52.616774Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "impossible-allowance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T08:28:39.321163Z",
     "start_time": "2021-04-16T08:28:39.297447Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "normal-application",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T08:28:41.192067Z",
     "start_time": "2021-04-16T08:28:40.888163Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "painted-findings",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T08:28:45.255700Z",
     "start_time": "2021-04-16T08:28:45.249329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-detector",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
