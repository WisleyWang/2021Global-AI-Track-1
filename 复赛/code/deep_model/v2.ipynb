{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unable-israel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:50:59.799951Z",
     "start_time": "2021-04-15T11:50:59.785652Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import Model\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Embedding, Dense, CuDNNLSTM,CuDNNGRU, Bidirectional,SpatialDropout1D,Input,\\\n",
    "GlobalAveragePooling1D,GlobalMaxPooling1D,Conv1D,concatenate,Dropout,Activation,BatchNormalization,Concatenate,Add,\\\n",
    "MaxPooling1D,Flatten,AveragePooling1D\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "import gensim, logging\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.model_selection import KFold,StratifiedShuffleSplit,StratifiedKFold\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pregnant-ozone",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:51:00.470682Z",
     "start_time": "2021-04-15T11:51:00.357696Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../tcdata/train.csv',header=None)\n",
    "test=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "# test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "data=pd.concat([train,test],axis=0)\n",
    "data[1]=data[1].apply(lambda x:x.strip().replace('|',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noble-manhattan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:51:01.644121Z",
     "start_time": "2021-04-15T11:51:01.549503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noncase label1: 0\n",
      "noncase label2: 8247\n"
     ]
    }
   ],
   "source": [
    "## 制作标签\n",
    "### 创建训练集标签 \n",
    "train_labels1=np.zeros((len(train),17),dtype='int8')\n",
    "noncase=0\n",
    "for cnt,i in enumerate(train[2]):\n",
    "    if i:\n",
    "        lab=[int(x.replace('|','').strip()) for x in i.split(' ') if x and x!='|']\n",
    "        for l in lab:\n",
    "            train_labels1[cnt,l]=1\n",
    "    else:\n",
    "        noncase+=1\n",
    "print('noncase label1:',noncase)\n",
    "#----------------------------------\n",
    "noncase=0\n",
    "train_labels2=np.zeros((len(train),12),dtype='int8')\n",
    "for cnt,i in enumerate(train[3]):\n",
    "    if pd.notna(i):\n",
    "        lab=[int(x.replace('|','').strip()) for x in i.split(' ') if x and x!='|']\n",
    "        for l in lab:\n",
    "            train_labels2[cnt,l]=1\n",
    "    else:\n",
    "        noncase+=1\n",
    "print('noncase label2:',noncase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specified-hebrew",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:51:05.925877Z",
     "start_time": "2021-04-15T11:51:05.917721Z"
    }
   },
   "outputs": [],
   "source": [
    "cate_num=train_labels1.sum(1)+train_labels2.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "floral-thickness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:51:06.162254Z",
     "start_time": "2021-04-15T11:51:06.156541Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels=np.concatenate([train_labels1,train_labels2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "english-burlington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:02:19.297605Z",
     "start_time": "2021-04-15T06:02:19.227226Z"
    }
   },
   "outputs": [],
   "source": [
    "## 生成全部的训练文本\n",
    "data1=pd.read_csv('../tcdata/track1_round1_train_20210222.csv',header=None)\n",
    "data2=pd.read_csv('../tcdata/track1_round1_testA_20210222.csv',header=None)\n",
    "data3=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "text_data=pd.concat([train,data1,data2,data3],axis=0)\n",
    "text_data[1]=text_data[1].apply(lambda x:x.strip().replace('|',''))\n",
    "text_data[1].to_csv('../tmp/all_data.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "overhead-plenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:07:16.368462Z",
     "start_time": "2021-04-15T06:07:16.288461Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data=pd.read_csv('../tmp/all_data.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "certified-afternoon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T10:46:24.265950Z",
     "start_time": "2021-04-15T10:46:24.262281Z"
    }
   },
   "outputs": [],
   "source": [
    "# w2v=Word2Vec(text_data[0].apply(lambda x:x.split(' ')).tolist(),size=128, window=8, iter=30, min_count=2,\n",
    "#                      sg=1, sample=0.002, workers=6 , seed=1017)\n",
    "\n",
    "# w2v.wv.save_word2vec_format('../tmp/w2v_128.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dental-thing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:51:13.804321Z",
     "start_time": "2021-04-15T11:51:12.566614Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False, char_level=False, split=' ')\n",
    "tokenizer.fit_on_texts(data[1].tolist())\n",
    "seq = tokenizer.texts_to_sequences(data[1].tolist())\n",
    "# 分训练和测试集合\n",
    "seq = pad_sequences(seq, maxlen=128, value=0)\n",
    "train_seq=np.asarray(seq[:len(train)])\n",
    "test_seq=seq[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "governmental-netscape",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:51:13.935687Z",
     "start_time": "2021-04-15T11:51:13.806165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:13,808 : INFO : loading projection weights from ../tmp/w2v_128.txt\n",
      "2021-04-15 19:51:13,918 : INFO : loaded (859, 128) matrix from ../tmp/w2v_128.txt\n",
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(859, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix=np.zeros((len(tokenizer.word_index)+1,128))\n",
    "w2v=gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        '../tmp/w2v_128.txt', binary=False)\n",
    "\n",
    "for word in tokenizer.word_index:\n",
    "    if word not in w2v.wv.vocab:\n",
    "        continue\n",
    "    embedding_matrix[tokenizer.word_index[word]] = w2v[word]\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compact-bearing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:51:15.796328Z",
     "start_time": "2021-04-15T11:51:15.752013Z"
    }
   },
   "outputs": [],
   "source": [
    "def NN_huaweiv1(maxlen,embedding_matrix=None,class_num1=17,class_num2=12):    \n",
    "    emb_layer = Embedding(\n",
    "       embedding_matrix.shape[0], embedding_matrix.shape[1],input_length=maxlen,weights=[embedding_matrix],trainable=False,\n",
    "    )\n",
    "    seq1 = Input(shape=(maxlen,)) \n",
    "    \n",
    "    x1 = emb_layer(seq1)\n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "    lstm_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    gru_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    cnn1d_layer=Conv1D(64, kernel_size=3, padding=\"same\", kernel_initializer=\"he_uniform\")\n",
    "    x1 = sdrop(x1)\n",
    "    lstm1 = lstm_layer(x1)\n",
    "    gru1 = gru_layer(lstm1)\n",
    "    att_1 = Attention(maxlen)(lstm1)\n",
    "    att_2 = Attention(maxlen)(gru1)\n",
    "    cnn1 = cnn1d_layer(lstm1)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()\n",
    "    max_pool = GlobalMaxPooling1D()\n",
    "\n",
    "    x1=concatenate([att_1,att_2,Attention(maxlen)(cnn1),avg_pool(cnn1),max_pool(cnn1)])\n",
    "\n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(128)(x1))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(64)(x)))\n",
    "    pred1_d = Dense(class_num1)(x)\n",
    "    pred1=Activation(activation='sigmoid',name='pred1')(pred1_d)\n",
    "    \n",
    "    y=concatenate([x1,x])\n",
    "    y = Activation(activation=\"relu\")(BatchNormalization()(Dense(64)(x)))\n",
    "    pred2_d=Dense(class_num2)(y)\n",
    "    pred2=Activation(activation='sigmoid',name='pred2')(pred2_d)\n",
    "    \n",
    "    z=Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(128)(x1))))\n",
    "    z=concatenate([pred1_d,pred2_d,z])\n",
    "    pred3= Dense(class_num1+class_num2, activation='sigmoid',name='pred3')(z)\n",
    "    model = Model(inputs=seq1, outputs=[pred1,pred2,pred3])\n",
    "    return model\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "#         print('-------------',type(input_shape))\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return -K.mean(y_true*K.log(y_pred+1.e-7)+(1-y_true)*K.log(1-y_pred+1.e-7))*10\n",
    "class Lookahead(object):\n",
    "    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=5, alpha=0.5):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.count = 0\n",
    "\n",
    "    def inject(self, model):\n",
    "        \"\"\"Inject the Lookahead algorithm for the given model.\n",
    "        The following code is modified from keras's _make_train_function method.\n",
    "        See: https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497\n",
    "        \"\"\"\n",
    "        if not hasattr(model, 'train_function'):\n",
    "            raise RuntimeError('You must compile your model before using it.')\n",
    "\n",
    "        model._check_trainable_weights_consistency()\n",
    "\n",
    "        if model.train_function is None:\n",
    "            inputs = (model._feed_inputs +\n",
    "                      model._feed_targets +\n",
    "                      model._feed_sample_weights)\n",
    "            if model._uses_dynamic_learning_phase():\n",
    "                inputs += [K.learning_phase()]\n",
    "            fast_params = model._collected_trainable_weights\n",
    "\n",
    "            with K.name_scope('training'):\n",
    "                with K.name_scope(model.optimizer.__class__.__name__):\n",
    "                    training_updates = model.optimizer.get_updates(\n",
    "                        params=fast_params,\n",
    "                        loss=model.total_loss)\n",
    "                    slow_params = [K.variable(p) for p in fast_params]\n",
    "                fast_updates = (model.updates +\n",
    "                                training_updates +\n",
    "                                model.metrics_updates)\n",
    "\n",
    "                slow_updates, copy_updates = [], []\n",
    "                for p, q in zip(fast_params, slow_params):\n",
    "                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n",
    "                    copy_updates.append(K.update(p, q))\n",
    "\n",
    "                # Gets loss and metrics. Updates weights at each call.\n",
    "                fast_train_function = K.function(\n",
    "                    inputs,\n",
    "                    [model.total_loss] + model.metrics_tensors,\n",
    "                    updates=fast_updates,\n",
    "                    name='fast_train_function',\n",
    "                    **model._function_kwargs)\n",
    "\n",
    "                def F(inputs):\n",
    "                    self.count += 1\n",
    "                    R = fast_train_function(inputs)\n",
    "                    if self.count % self.k == 0:\n",
    "                        K.batch_get_value(slow_updates)\n",
    "                        K.batch_get_value(copy_updates)\n",
    "                    return R\n",
    "                \n",
    "                model.train_function = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accurate-butler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T11:55:57.033057Z",
     "start_time": "2021-04-15T11:51:15.964807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Build model...\n",
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "2021-04-15 19:51:15,986 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:16,007 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:16,012 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:16,025 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:16,026 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:17,029 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:18,921 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:18,948 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:2618: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:51:22,146 : WARNING : From /home/lichangyv/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:2618: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 12s 674us/step - loss: 1.1442 - pred1_loss: 0.4361 - pred2_loss: 0.4410 - pred3_loss: 0.2671 - pred1_acc: 0.8144 - pred1_mean_pred: 4.3605 - pred2_acc: 0.8247 - pred2_mean_pred: 4.4103 - pred3_acc: 0.9009 - pred3_mean_pred: 2.6708 - val_loss: 0.5314 - val_pred1_loss: 0.2184 - val_pred2_loss: 0.1846 - val_pred3_loss: 0.1283 - val_pred1_acc: 0.9394 - val_pred1_mean_pred: 2.1844 - val_pred2_acc: 0.9623 - val_pred2_mean_pred: 1.8463 - val_pred3_acc: 0.9547 - val_pred3_mean_pred: 1.2833\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53140, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 7s 390us/step - loss: 0.4774 - pred1_loss: 0.2002 - pred2_loss: 0.1612 - pred3_loss: 0.1159 - pred1_acc: 0.9455 - pred1_mean_pred: 2.0022 - pred2_acc: 0.9703 - pred2_mean_pred: 1.6119 - pred3_acc: 0.9589 - pred3_mean_pred: 1.1594 - val_loss: 0.3793 - val_pred1_loss: 0.1596 - val_pred2_loss: 0.1217 - val_pred3_loss: 0.0980 - val_pred1_acc: 0.9528 - val_pred1_mean_pred: 1.5956 - val_pred2_acc: 0.9736 - val_pred2_mean_pred: 1.2168 - val_pred3_acc: 0.9649 - val_pred3_mean_pred: 0.9801\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53140 to 0.37925, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 7s 400us/step - loss: 0.3455 - pred1_loss: 0.1473 - pred2_loss: 0.1026 - pred3_loss: 0.0955 - pred1_acc: 0.9521 - pred1_mean_pred: 1.4734 - pred2_acc: 0.9748 - pred2_mean_pred: 1.0264 - pred3_acc: 0.9644 - pred3_mean_pred: 0.9549 - val_loss: 0.3062 - val_pred1_loss: 0.1311 - val_pred2_loss: 0.0905 - val_pred3_loss: 0.0846 - val_pred1_acc: 0.9551 - val_pred1_mean_pred: 1.3107 - val_pred2_acc: 0.9766 - val_pred2_mean_pred: 0.9049 - val_pred3_acc: 0.9671 - val_pred3_mean_pred: 0.8459\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37925 to 0.30615, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 7s 398us/step - loss: 0.2879 - pred1_loss: 0.1251 - pred2_loss: 0.0790 - pred3_loss: 0.0838 - pred1_acc: 0.9556 - pred1_mean_pred: 1.2514 - pred2_acc: 0.9777 - pred2_mean_pred: 0.7896 - pred3_acc: 0.9674 - pred3_mean_pred: 0.8382 - val_loss: 0.2746 - val_pred1_loss: 0.1190 - val_pred2_loss: 0.0773 - val_pred3_loss: 0.0783 - val_pred1_acc: 0.9583 - val_pred1_mean_pred: 1.1898 - val_pred2_acc: 0.9799 - val_pred2_mean_pred: 0.7735 - val_pred3_acc: 0.9690 - val_pred3_mean_pred: 0.7830\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30615 to 0.27463, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 7s 394us/step - loss: 0.2513 - pred1_loss: 0.1116 - pred2_loss: 0.0645 - pred3_loss: 0.0752 - pred1_acc: 0.9578 - pred1_mean_pred: 1.1159 - pred2_acc: 0.9811 - pred2_mean_pred: 0.6448 - pred3_acc: 0.9700 - pred3_mean_pred: 0.7519 - val_loss: 0.2290 - val_pred1_loss: 0.1025 - val_pred2_loss: 0.0576 - val_pred3_loss: 0.0688 - val_pred1_acc: 0.9602 - val_pred1_mean_pred: 1.0255 - val_pred2_acc: 0.9830 - val_pred2_mean_pred: 0.5757 - val_pred3_acc: 0.9729 - val_pred3_mean_pred: 0.6883\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.27463 to 0.22896, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 6/100\n",
      "18000/18000 [==============================] - 7s 396us/step - loss: 0.2264 - pred1_loss: 0.1027 - pred2_loss: 0.0557 - pred3_loss: 0.0680 - pred1_acc: 0.9596 - pred1_mean_pred: 1.0274 - pred2_acc: 0.9831 - pred2_mean_pred: 0.5573 - pred3_acc: 0.9732 - pred3_mean_pred: 0.6795 - val_loss: 0.2063 - val_pred1_loss: 0.0950 - val_pred2_loss: 0.0530 - val_pred3_loss: 0.0583 - val_pred1_acc: 0.9623 - val_pred1_mean_pred: 0.9504 - val_pred2_acc: 0.9848 - val_pred2_mean_pred: 0.5296 - val_pred3_acc: 0.9791 - val_pred3_mean_pred: 0.5833\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.22896 to 0.20632, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 7/100\n",
      "18000/18000 [==============================] - 7s 406us/step - loss: 0.2002 - pred1_loss: 0.0935 - pred2_loss: 0.0494 - pred3_loss: 0.0573 - pred1_acc: 0.9632 - pred1_mean_pred: 0.9355 - pred2_acc: 0.9849 - pred2_mean_pred: 0.4943 - pred3_acc: 0.9788 - pred3_mean_pred: 0.5726 - val_loss: 0.1818 - val_pred1_loss: 0.0854 - val_pred2_loss: 0.0459 - val_pred3_loss: 0.0505 - val_pred1_acc: 0.9679 - val_pred1_mean_pred: 0.8539 - val_pred2_acc: 0.9859 - val_pred2_mean_pred: 0.4592 - val_pred3_acc: 0.9823 - val_pred3_mean_pred: 0.5045\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20632 to 0.18176, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 8/100\n",
      "18000/18000 [==============================] - 7s 397us/step - loss: 0.1781 - pred1_loss: 0.0836 - pred2_loss: 0.0442 - pred3_loss: 0.0503 - pred1_acc: 0.9687 - pred1_mean_pred: 0.8361 - pred2_acc: 0.9861 - pred2_mean_pred: 0.4424 - pred3_acc: 0.9822 - pred3_mean_pred: 0.5028 - val_loss: 0.1654 - val_pred1_loss: 0.0770 - val_pred2_loss: 0.0427 - val_pred3_loss: 0.0457 - val_pred1_acc: 0.9737 - val_pred1_mean_pred: 0.7701 - val_pred2_acc: 0.9870 - val_pred2_mean_pred: 0.4268 - val_pred3_acc: 0.9846 - val_pred3_mean_pred: 0.4570\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18176 to 0.16539, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 9/100\n",
      "18000/18000 [==============================] - 7s 405us/step - loss: 0.1597 - pred1_loss: 0.0742 - pred2_loss: 0.0402 - pred3_loss: 0.0453 - pred1_acc: 0.9737 - pred1_mean_pred: 0.7421 - pred2_acc: 0.9874 - pred2_mean_pred: 0.4024 - pred3_acc: 0.9841 - pred3_mean_pred: 0.4529 - val_loss: 0.1548 - val_pred1_loss: 0.0691 - val_pred2_loss: 0.0414 - val_pred3_loss: 0.0443 - val_pred1_acc: 0.9768 - val_pred1_mean_pred: 0.6909 - val_pred2_acc: 0.9867 - val_pred2_mean_pred: 0.4141 - val_pred3_acc: 0.9849 - val_pred3_mean_pred: 0.4429\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.16539 to 0.15480, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 10/100\n",
      "18000/18000 [==============================] - 7s 404us/step - loss: 0.1465 - pred1_loss: 0.0672 - pred2_loss: 0.0375 - pred3_loss: 0.0417 - pred1_acc: 0.9764 - pred1_mean_pred: 0.6724 - pred2_acc: 0.9879 - pred2_mean_pred: 0.3752 - pred3_acc: 0.9852 - pred3_mean_pred: 0.4174 - val_loss: 0.1389 - val_pred1_loss: 0.0621 - val_pred2_loss: 0.0371 - val_pred3_loss: 0.0397 - val_pred1_acc: 0.9791 - val_pred1_mean_pred: 0.6208 - val_pred2_acc: 0.9882 - val_pred2_mean_pred: 0.3706 - val_pred3_acc: 0.9861 - val_pred3_mean_pred: 0.3972\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15480 to 0.13886, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 11/100\n",
      "18000/18000 [==============================] - 7s 408us/step - loss: 0.1353 - pred1_loss: 0.0615 - pred2_loss: 0.0346 - pred3_loss: 0.0391 - pred1_acc: 0.9786 - pred1_mean_pred: 0.6150 - pred2_acc: 0.9890 - pred2_mean_pred: 0.3464 - pred3_acc: 0.9862 - pred3_mean_pred: 0.3912 - val_loss: 0.1323 - val_pred1_loss: 0.0578 - val_pred2_loss: 0.0361 - val_pred3_loss: 0.0384 - val_pred1_acc: 0.9819 - val_pred1_mean_pred: 0.5780 - val_pred2_acc: 0.9891 - val_pred2_mean_pred: 0.3615 - val_pred3_acc: 0.9869 - val_pred3_mean_pred: 0.3836\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.13886 to 0.13230, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 12/100\n",
      "18000/18000 [==============================] - 7s 401us/step - loss: 0.1239 - pred1_loss: 0.0564 - pred2_loss: 0.0317 - pred3_loss: 0.0358 - pred1_acc: 0.9803 - pred1_mean_pred: 0.5640 - pred2_acc: 0.9901 - pred2_mean_pred: 0.3168 - pred3_acc: 0.9874 - pred3_mean_pred: 0.3582 - val_loss: 0.1260 - val_pred1_loss: 0.0544 - val_pred2_loss: 0.0345 - val_pred3_loss: 0.0371 - val_pred1_acc: 0.9822 - val_pred1_mean_pred: 0.5439 - val_pred2_acc: 0.9889 - val_pred2_mean_pred: 0.3455 - val_pred3_acc: 0.9873 - val_pred3_mean_pred: 0.3707\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.13230 to 0.12600, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 13/100\n",
      "18000/18000 [==============================] - 7s 393us/step - loss: 0.1181 - pred1_loss: 0.0531 - pred2_loss: 0.0306 - pred3_loss: 0.0344 - pred1_acc: 0.9811 - pred1_mean_pred: 0.5313 - pred2_acc: 0.9900 - pred2_mean_pred: 0.3060 - pred3_acc: 0.9878 - pred3_mean_pred: 0.3437 - val_loss: 0.1232 - val_pred1_loss: 0.0528 - val_pred2_loss: 0.0325 - val_pred3_loss: 0.0379 - val_pred1_acc: 0.9820 - val_pred1_mean_pred: 0.5276 - val_pred2_acc: 0.9896 - val_pred2_mean_pred: 0.3254 - val_pred3_acc: 0.9869 - val_pred3_mean_pred: 0.3787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_loss improved from 0.12600 to 0.12317, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 14/100\n",
      "18000/18000 [==============================] - 7s 405us/step - loss: 0.1100 - pred1_loss: 0.0499 - pred2_loss: 0.0277 - pred3_loss: 0.0324 - pred1_acc: 0.9825 - pred1_mean_pred: 0.4991 - pred2_acc: 0.9910 - pred2_mean_pred: 0.2769 - pred3_acc: 0.9885 - pred3_mean_pred: 0.3244 - val_loss: 0.1197 - val_pred1_loss: 0.0495 - val_pred2_loss: 0.0332 - val_pred3_loss: 0.0369 - val_pred1_acc: 0.9825 - val_pred1_mean_pred: 0.4953 - val_pred2_acc: 0.9889 - val_pred2_mean_pred: 0.3322 - val_pred3_acc: 0.9874 - val_pred3_mean_pred: 0.3691\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.12317 to 0.11966, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 15/100\n",
      "18000/18000 [==============================] - 7s 401us/step - loss: 0.1038 - pred1_loss: 0.0471 - pred2_loss: 0.0263 - pred3_loss: 0.0305 - pred1_acc: 0.9835 - pred1_mean_pred: 0.4708 - pred2_acc: 0.9913 - pred2_mean_pred: 0.2629 - pred3_acc: 0.9893 - pred3_mean_pred: 0.3045 - val_loss: 0.1102 - val_pred1_loss: 0.0461 - val_pred2_loss: 0.0304 - val_pred3_loss: 0.0337 - val_pred1_acc: 0.9847 - val_pred1_mean_pred: 0.4611 - val_pred2_acc: 0.9897 - val_pred2_mean_pred: 0.3041 - val_pred3_acc: 0.9882 - val_pred3_mean_pred: 0.3372\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11966 to 0.11024, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 16/100\n",
      "18000/18000 [==============================] - 7s 408us/step - loss: 0.0981 - pred1_loss: 0.0446 - pred2_loss: 0.0246 - pred3_loss: 0.0289 - pred1_acc: 0.9842 - pred1_mean_pred: 0.4457 - pred2_acc: 0.9921 - pred2_mean_pred: 0.2464 - pred3_acc: 0.9897 - pred3_mean_pred: 0.2886 - val_loss: 0.1077 - val_pred1_loss: 0.0450 - val_pred2_loss: 0.0291 - val_pred3_loss: 0.0336 - val_pred1_acc: 0.9853 - val_pred1_mean_pred: 0.4499 - val_pred2_acc: 0.9908 - val_pred2_mean_pred: 0.2914 - val_pred3_acc: 0.9886 - val_pred3_mean_pred: 0.3357\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11024 to 0.10769, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 17/100\n",
      "18000/18000 [==============================] - 7s 396us/step - loss: 0.0935 - pred1_loss: 0.0424 - pred2_loss: 0.0235 - pred3_loss: 0.0276 - pred1_acc: 0.9850 - pred1_mean_pred: 0.4242 - pred2_acc: 0.9922 - pred2_mean_pred: 0.2347 - pred3_acc: 0.9901 - pred3_mean_pred: 0.2764 - val_loss: 0.1043 - val_pred1_loss: 0.0439 - val_pred2_loss: 0.0275 - val_pred3_loss: 0.0329 - val_pred1_acc: 0.9847 - val_pred1_mean_pred: 0.4391 - val_pred2_acc: 0.9910 - val_pred2_mean_pred: 0.2751 - val_pred3_acc: 0.9888 - val_pred3_mean_pred: 0.3290\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.10769 to 0.10432, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 18/100\n",
      "18000/18000 [==============================] - 7s 398us/step - loss: 0.0889 - pred1_loss: 0.0406 - pred2_loss: 0.0221 - pred3_loss: 0.0262 - pred1_acc: 0.9859 - pred1_mean_pred: 0.4058 - pred2_acc: 0.9927 - pred2_mean_pred: 0.2206 - pred3_acc: 0.9908 - pred3_mean_pred: 0.2623 - val_loss: 0.1008 - val_pred1_loss: 0.0417 - val_pred2_loss: 0.0267 - val_pred3_loss: 0.0324 - val_pred1_acc: 0.9858 - val_pred1_mean_pred: 0.4171 - val_pred2_acc: 0.9918 - val_pred2_mean_pred: 0.2670 - val_pred3_acc: 0.9890 - val_pred3_mean_pred: 0.3238\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10432 to 0.10079, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 19/100\n",
      "18000/18000 [==============================] - 7s 404us/step - loss: 0.0834 - pred1_loss: 0.0380 - pred2_loss: 0.0206 - pred3_loss: 0.0248 - pred1_acc: 0.9867 - pred1_mean_pred: 0.3796 - pred2_acc: 0.9932 - pred2_mean_pred: 0.2062 - pred3_acc: 0.9912 - pred3_mean_pred: 0.2478 - val_loss: 0.1115 - val_pred1_loss: 0.0467 - val_pred2_loss: 0.0270 - val_pred3_loss: 0.0378 - val_pred1_acc: 0.9834 - val_pred1_mean_pred: 0.4672 - val_pred2_acc: 0.9913 - val_pred2_mean_pred: 0.2704 - val_pred3_acc: 0.9872 - val_pred3_mean_pred: 0.3776\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10079\n",
      "Epoch 20/100\n",
      "18000/18000 [==============================] - 7s 405us/step - loss: 0.0805 - pred1_loss: 0.0369 - pred2_loss: 0.0196 - pred3_loss: 0.0239 - pred1_acc: 0.9869 - pred1_mean_pred: 0.3695 - pred2_acc: 0.9937 - pred2_mean_pred: 0.1965 - pred3_acc: 0.9915 - pred3_mean_pred: 0.2389 - val_loss: 0.0973 - val_pred1_loss: 0.0393 - val_pred2_loss: 0.0270 - val_pred3_loss: 0.0310 - val_pred1_acc: 0.9871 - val_pred1_mean_pred: 0.3929 - val_pred2_acc: 0.9913 - val_pred2_mean_pred: 0.2701 - val_pred3_acc: 0.9893 - val_pred3_mean_pred: 0.3096\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10079 to 0.09726, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 21/100\n",
      "18000/18000 [==============================] - 7s 412us/step - loss: 0.0762 - pred1_loss: 0.0352 - pred2_loss: 0.0185 - pred3_loss: 0.0224 - pred1_acc: 0.9877 - pred1_mean_pred: 0.3523 - pred2_acc: 0.9938 - pred2_mean_pred: 0.1854 - pred3_acc: 0.9921 - pred3_mean_pred: 0.2244 - val_loss: 0.0945 - val_pred1_loss: 0.0390 - val_pred2_loss: 0.0255 - val_pred3_loss: 0.0300 - val_pred1_acc: 0.9867 - val_pred1_mean_pred: 0.3904 - val_pred2_acc: 0.9911 - val_pred2_mean_pred: 0.2549 - val_pred3_acc: 0.9896 - val_pred3_mean_pred: 0.3001\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09726 to 0.09455, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 22/100\n",
      "18000/18000 [==============================] - 7s 410us/step - loss: 0.0746 - pred1_loss: 0.0345 - pred2_loss: 0.0182 - pred3_loss: 0.0219 - pred1_acc: 0.9877 - pred1_mean_pred: 0.3449 - pred2_acc: 0.9942 - pred2_mean_pred: 0.1823 - pred3_acc: 0.9920 - pred3_mean_pred: 0.2189 - val_loss: 0.0972 - val_pred1_loss: 0.0395 - val_pred2_loss: 0.0262 - val_pred3_loss: 0.0315 - val_pred1_acc: 0.9864 - val_pred1_mean_pred: 0.3951 - val_pred2_acc: 0.9916 - val_pred2_mean_pred: 0.2621 - val_pred3_acc: 0.9894 - val_pred3_mean_pred: 0.3152\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.09455\n",
      "Epoch 23/100\n",
      "18000/18000 [==============================] - 7s 411us/step - loss: 0.0708 - pred1_loss: 0.0330 - pred2_loss: 0.0170 - pred3_loss: 0.0208 - pred1_acc: 0.9883 - pred1_mean_pred: 0.3297 - pred2_acc: 0.9944 - pred2_mean_pred: 0.1705 - pred3_acc: 0.9925 - pred3_mean_pred: 0.2076 - val_loss: 0.0929 - val_pred1_loss: 0.0373 - val_pred2_loss: 0.0252 - val_pred3_loss: 0.0305 - val_pred1_acc: 0.9875 - val_pred1_mean_pred: 0.3726 - val_pred2_acc: 0.9923 - val_pred2_mean_pred: 0.2519 - val_pred3_acc: 0.9898 - val_pred3_mean_pred: 0.3049\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09455 to 0.09294, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 24/100\n",
      "18000/18000 [==============================] - 8s 417us/step - loss: 0.0672 - pred1_loss: 0.0314 - pred2_loss: 0.0161 - pred3_loss: 0.0196 - pred1_acc: 0.9888 - pred1_mean_pred: 0.3144 - pred2_acc: 0.9947 - pred2_mean_pred: 0.1612 - pred3_acc: 0.9928 - pred3_mean_pred: 0.1960 - val_loss: 0.0939 - val_pred1_loss: 0.0376 - val_pred2_loss: 0.0253 - val_pred3_loss: 0.0310 - val_pred1_acc: 0.9876 - val_pred1_mean_pred: 0.3760 - val_pred2_acc: 0.9918 - val_pred2_mean_pred: 0.2533 - val_pred3_acc: 0.9896 - val_pred3_mean_pred: 0.3101\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09294\n",
      "Epoch 25/100\n",
      "18000/18000 [==============================] - 8s 417us/step - loss: 0.0644 - pred1_loss: 0.0308 - pred2_loss: 0.0148 - pred3_loss: 0.0189 - pred1_acc: 0.9890 - pred1_mean_pred: 0.3077 - pred2_acc: 0.9952 - pred2_mean_pred: 0.1479 - pred3_acc: 0.9931 - pred3_mean_pred: 0.1885 - val_loss: 0.0887 - val_pred1_loss: 0.0355 - val_pred2_loss: 0.0237 - val_pred3_loss: 0.0295 - val_pred1_acc: 0.9882 - val_pred1_mean_pred: 0.3552 - val_pred2_acc: 0.9925 - val_pred2_mean_pred: 0.2367 - val_pred3_acc: 0.9902 - val_pred3_mean_pred: 0.2948\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09294 to 0.08867, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 26/100\n",
      "18000/18000 [==============================] - 8s 425us/step - loss: 0.0622 - pred1_loss: 0.0292 - pred2_loss: 0.0148 - pred3_loss: 0.0182 - pred1_acc: 0.9896 - pred1_mean_pred: 0.2922 - pred2_acc: 0.9951 - pred2_mean_pred: 0.1481 - pred3_acc: 0.9933 - pred3_mean_pred: 0.1817 - val_loss: 0.0879 - val_pred1_loss: 0.0346 - val_pred2_loss: 0.0235 - val_pred3_loss: 0.0298 - val_pred1_acc: 0.9884 - val_pred1_mean_pred: 0.3464 - val_pred2_acc: 0.9926 - val_pred2_mean_pred: 0.2349 - val_pred3_acc: 0.9903 - val_pred3_mean_pred: 0.2978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_loss improved from 0.08867 to 0.08791, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 27/100\n",
      "18000/18000 [==============================] - 8s 421us/step - loss: 0.0610 - pred1_loss: 0.0292 - pred2_loss: 0.0140 - pred3_loss: 0.0178 - pred1_acc: 0.9895 - pred1_mean_pred: 0.2923 - pred2_acc: 0.9954 - pred2_mean_pred: 0.1403 - pred3_acc: 0.9935 - pred3_mean_pred: 0.1777 - val_loss: 0.0876 - val_pred1_loss: 0.0353 - val_pred2_loss: 0.0227 - val_pred3_loss: 0.0296 - val_pred1_acc: 0.9884 - val_pred1_mean_pred: 0.3533 - val_pred2_acc: 0.9930 - val_pred2_mean_pred: 0.2265 - val_pred3_acc: 0.9904 - val_pred3_mean_pred: 0.2957\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08791 to 0.08756, saving model to ../model_weight/V2_0.h5\n",
      "Epoch 28/100\n",
      "18000/18000 [==============================] - 8s 422us/step - loss: 0.0580 - pred1_loss: 0.0278 - pred2_loss: 0.0135 - pred3_loss: 0.0168 - pred1_acc: 0.9899 - pred1_mean_pred: 0.2776 - pred2_acc: 0.9956 - pred2_mean_pred: 0.1348 - pred3_acc: 0.9939 - pred3_mean_pred: 0.1679 - val_loss: 0.0928 - val_pred1_loss: 0.0358 - val_pred2_loss: 0.0252 - val_pred3_loss: 0.0319 - val_pred1_acc: 0.9874 - val_pred1_mean_pred: 0.3576 - val_pred2_acc: 0.9917 - val_pred2_mean_pred: 0.2519 - val_pred3_acc: 0.9896 - val_pred3_mean_pred: 0.3187\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.08756\n",
      "Epoch 29/100\n",
      "18000/18000 [==============================] - 8s 426us/step - loss: 0.0571 - pred1_loss: 0.0271 - pred2_loss: 0.0135 - pred3_loss: 0.0165 - pred1_acc: 0.9903 - pred1_mean_pred: 0.2712 - pred2_acc: 0.9956 - pred2_mean_pred: 0.1347 - pred3_acc: 0.9939 - pred3_mean_pred: 0.1648 - val_loss: 0.0943 - val_pred1_loss: 0.0382 - val_pred2_loss: 0.0237 - val_pred3_loss: 0.0324 - val_pred1_acc: 0.9869 - val_pred1_mean_pred: 0.3824 - val_pred2_acc: 0.9926 - val_pred2_mean_pred: 0.2371 - val_pred3_acc: 0.9896 - val_pred3_mean_pred: 0.3238\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.08756\n",
      "Fold 1\n",
      "Build model...\n",
      "Train...\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 12s 660us/step - loss: 1.2231 - pred1_loss: 0.4622 - pred2_loss: 0.4633 - pred3_loss: 0.2977 - pred1_acc: 0.8104 - pred1_mean_pred: 4.6217 - pred2_acc: 0.8118 - pred2_mean_pred: 4.6325 - pred3_acc: 0.8836 - pred3_mean_pred: 2.9766 - val_loss: 0.5645 - val_pred1_loss: 0.2332 - val_pred2_loss: 0.1971 - val_pred3_loss: 0.1342 - val_pred1_acc: 0.9393 - val_pred1_mean_pred: 2.3319 - val_pred2_acc: 0.9611 - val_pred2_mean_pred: 1.9710 - val_pred3_acc: 0.9542 - val_pred3_mean_pred: 1.3419\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56448, saving model to ../model_weight/V2_1.h5\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 8s 425us/step - loss: 0.5065 - pred1_loss: 0.2129 - pred2_loss: 0.1711 - pred3_loss: 0.1226 - pred1_acc: 0.9429 - pred1_mean_pred: 2.1286 - pred2_acc: 0.9685 - pred2_mean_pred: 1.7111 - pred3_acc: 0.9572 - pred3_mean_pred: 1.2255 - val_loss: 0.4036 - val_pred1_loss: 0.1736 - val_pred2_loss: 0.1272 - val_pred3_loss: 0.1029 - val_pred1_acc: 0.9504 - val_pred1_mean_pred: 1.7361 - val_pred2_acc: 0.9747 - val_pred2_mean_pred: 1.2716 - val_pred3_acc: 0.9628 - val_pred3_mean_pred: 1.0287\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56448 to 0.40363, saving model to ../model_weight/V2_1.h5\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 8s 432us/step - loss: 0.3593 - pred1_loss: 0.1540 - pred2_loss: 0.1071 - pred3_loss: 0.0982 - pred1_acc: 0.9511 - pred1_mean_pred: 1.5404 - pred2_acc: 0.9746 - pred2_mean_pred: 1.0713 - pred3_acc: 0.9635 - pred3_mean_pred: 0.9815 - val_loss: 0.3190 - val_pred1_loss: 0.1406 - val_pred2_loss: 0.0928 - val_pred3_loss: 0.0856 - val_pred1_acc: 0.9544 - val_pred1_mean_pred: 1.4058 - val_pred2_acc: 0.9776 - val_pred2_mean_pred: 0.9285 - val_pred3_acc: 0.9667 - val_pred3_mean_pred: 0.8557\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40363 to 0.31900, saving model to ../model_weight/V2_1.h5\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 8s 442us/step - loss: 0.2953 - pred1_loss: 0.1290 - pred2_loss: 0.0811 - pred3_loss: 0.0852 - pred1_acc: 0.9548 - pred1_mean_pred: 1.2898 - pred2_acc: 0.9781 - pred2_mean_pred: 0.8114 - pred3_acc: 0.9670 - pred3_mean_pred: 0.8523 - val_loss: 0.2738 - val_pred1_loss: 0.1217 - val_pred2_loss: 0.0716 - val_pred3_loss: 0.0805 - val_pred1_acc: 0.9569 - val_pred1_mean_pred: 1.2173 - val_pred2_acc: 0.9795 - val_pred2_mean_pred: 0.7157 - val_pred3_acc: 0.9676 - val_pred3_mean_pred: 0.8048\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31900 to 0.27377, saving model to ../model_weight/V2_1.h5\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 8s 445us/step - loss: 0.2565 - pred1_loss: 0.1144 - pred2_loss: 0.0664 - pred3_loss: 0.0757 - pred1_acc: 0.9572 - pred1_mean_pred: 1.1436 - pred2_acc: 0.9812 - pred2_mean_pred: 0.6643 - pred3_acc: 0.9701 - pred3_mean_pred: 0.7572 - val_loss: 0.2318 - val_pred1_loss: 0.1084 - val_pred2_loss: 0.0571 - val_pred3_loss: 0.0663 - val_pred1_acc: 0.9581 - val_pred1_mean_pred: 1.0843 - val_pred2_acc: 0.9834 - val_pred2_mean_pred: 0.5707 - val_pred3_acc: 0.9733 - val_pred3_mean_pred: 0.6630\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.27377 to 0.23181, saving model to ../model_weight/V2_1.h5\n",
      "Epoch 6/100\n",
      " 4608/18000 [======>.......................] - ETA: 5s - loss: 0.2394 - pred1_loss: 0.1084 - pred2_loss: 0.0604 - pred3_loss: 0.0706 - pred1_acc: 0.9586 - pred1_mean_pred: 1.0837 - pred2_acc: 0.9821 - pred2_mean_pred: 0.6043 - pred3_acc: 0.9723 - pred3_mean_pred: 0.7058"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f5ea4aa4314a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheck_point\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m               validation_data=(train_seq[val_idx],{'pred1':train_labels1[val_idx],'pred2':train_labels2[val_idx],'pred3':train_labels[val_idx]}))\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model_weight/%s_%d.h5'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f4801404e6e1>\u001b[0m in \u001b[0;36mF\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=100\n",
    "weight_name='V2'\n",
    "oof=np.zeros((len(train),29))\n",
    "tmp=0\n",
    "test_oof=np.zeros((len(test),29))\n",
    "\n",
    "folds=StratifiedKFold(n_splits=10,shuffle=True, random_state=2018) #2018\n",
    "for fold_n, (trn_idx, val_idx) in enumerate(folds.split(train,cate_num)):\n",
    "    print('Fold', fold_n)\n",
    "    print('Build model...')\n",
    "#     print('正样本比例:',train_labels[trn_idx].mean(0))\n",
    "    model=NN_huaweiv1(maxlen=128,embedding_matrix=embedding_matrix)\n",
    "#     model=multi_gpu_model(model,gpus=2)\n",
    "    model.compile('adam', ['binary_crossentropy','binary_crossentropy','binary_crossentropy'], metrics=['accuracy',mean_pred])\n",
    "#     lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "#     lookahead.inject(model) # add into model\n",
    "    print('Train...')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2,mode='min')\n",
    "    check_point=ModelCheckpoint('../model_weight/%s_%d.h5'%(weight_name,fold_n),monitor='val_loss',verbose=1, save_best_only=True,save_weights_only=True)\n",
    "\n",
    "    model.fit(train_seq[trn_idx],{'pred1':train_labels1[trn_idx],'pred2':train_labels2[trn_idx],'pred3':train_labels[trn_idx]},\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping,check_point],\n",
    "              validation_data=(train_seq[val_idx],{'pred1':train_labels1[val_idx],'pred2':train_labels2[val_idx],'pred3':train_labels[val_idx]}))\n",
    "    model.load_weights('../model_weight/%s_%d.h5'%(weight_name,fold_n))\n",
    "    _,_,oof[val_idx,:]= model.predict(train_seq[val_idx],batch_size=batch_size)\n",
    "    _,_,tmp_test_pred=model.predict(test_seq,batch_size=batch_size)\n",
    "    test_oof[:]=tmp_test_pred/folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "false-humanitarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:03:15.910141Z",
     "start_time": "2021-04-15T07:03:15.802681Z"
    }
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['report_ID']=test[0]\n",
    "sub['Prediction']=[ '|'+' '.join(['%.12f'%j for j in i]) for i in test_oof ]\n",
    "sub.to_csv('../result.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-neutral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
