{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fantastic-latex",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T05:03:54.154415Z",
     "start_time": "2021-05-10T05:03:54.075054Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer,AutoModelForPreTraining\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,AutoModel\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n",
    "from model.modeling_nezha import NeZhaForSequenceClassification,NeZhaPreTrainedModel,NeZhaModel,NeZhaForTokenClassification\n",
    "from model.configuration_nezha import NeZhaConfig\n",
    "import  torch.nn.functional as F\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertOutput,\n",
    "    BertPooler,\n",
    "    BertSelfOutput,\n",
    "    BertIntermediate,\n",
    "    BertOnlyMLMHead,\n",
    "    BertOnlyNSPHead,\n",
    "    BertPreTrainingHeads,\n",
    "    BERT_START_DOCSTRING,\n",
    "    BERT_INPUTS_DOCSTRING,\n",
    ")\n",
    "from transformers import MPNetModel,MPNetConfig,MPNetTokenizerFast,MPNetTokenizer,BertTokenizer\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "class NeZhaForSequenceClassification(NeZhaPreTrainedModel):\n",
    "    def __init__(self, config,model_name,num_labels1,num_labels2):\n",
    "        super().__init__(config)\n",
    "        self.num_labels1 = num_labels1\n",
    "        self.num_labels2=num_labels2\n",
    "        self.bert = MPNetModel(config).from_pretrained(model_name)\n",
    "        self.attn1=Attn(config.hidden_size)\n",
    "        self.attn2=Attn(config.hidden_size)\n",
    "        self.attn3=Attn(config.hidden_size)\n",
    "        self.attn4=Attn(config.hidden_size)\n",
    "        self.attn5=Attn(config.hidden_size)\n",
    "        self.attn6=Attn(config.hidden_size)\n",
    "        self.attn7=Attn(config.hidden_size)\n",
    "        self.attn8=Attn(config.hidden_size)\n",
    "        self.attn9=Attn(config.hidden_size)\n",
    "        self.attn10=Attn(config.hidden_size)\n",
    "        self.attn11=Attn(config.hidden_size)\n",
    "        \n",
    "        self.dropouts=nn.ModuleList([nn.Dropout(p) for p in np.linspace(0.1,0.4,3)])\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier1 = nn.Linear(config.hidden_size, self.num_labels1-10)#3,4,5,6,7,8,9,\n",
    "        self.classifier2 = nn.Linear(config.hidden_size,6) #10,11,13,14,15,17\n",
    "        self.classifier3 = nn.Linear(config.hidden_size, 1)#1\n",
    "        self.classifier4 = nn.Linear(config.hidden_size, 1) #2\n",
    "        self.classifier5 = nn.Linear(config.hidden_size, 1)#12 \n",
    "        self.classifier6 = nn.Linear(config.hidden_size, 1) #16\n",
    "\n",
    "\n",
    "        self.classifier7 = nn.Linear(config.hidden_size, self.num_labels2-7) #7,8,9,10,11\n",
    "        self.classifier8 = nn.Linear(config.hidden_size,4) #2,3,4,6\n",
    "        self.classifier9= nn.Linear(config.hidden_size,1) #1\n",
    "        self.classifier10= nn.Linear(config.hidden_size,1) # 5\n",
    "        self.classifier11= nn.Linear(config.hidden_size,1) # 12\n",
    "        \n",
    "        self.predict=nn.Sigmoid()\n",
    "#         self.init_weights()\n",
    "#         if True:\n",
    "#             for p in self.bert.parameters(): # 冻结所有bert层\n",
    "#                 p.requires_grad = False\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            labels1=None,\n",
    "        labels2=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "    Returns:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        import torch\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = self.bert(\n",
    "           input_ids= input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        att1=self.attn1(outputs[0])\n",
    "        att2=self.attn2(outputs[0])\n",
    "        att3=self.attn3(outputs[0])\n",
    "        att4=self.attn4(outputs[0])\n",
    "        att5=self.attn5(outputs[0])\n",
    "        att6=self.attn6(outputs[0])\n",
    "        att7=self.attn7(outputs[0])\n",
    "        att8=self.attn8(outputs[0])\n",
    "        att9=self.attn9(outputs[0])\n",
    "        att10=self.attn10(outputs[0])\n",
    "        att11=self.attn11(outputs[0])\n",
    "        \n",
    "        pooled_output1 = self.dropout(att1)\n",
    "        pooled_output2 = torch.stack([ dd(att2)for dd in self.dropouts],dim=0).mean(dim=0)#self.dropout(att2)\n",
    "        pooled_output3 =torch.stack([ dd(att3)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        pooled_output4 = torch.stack([ dd(att4)for dd in self.dropouts],dim=0).mean(dim=0)#self.dropout(att4)\n",
    "        pooled_output5 = torch.stack([ dd(att5)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        pooled_output6 = torch.stack([ dd(att6)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        pooled_output7 = self.dropout(att7)\n",
    "        pooled_output8 = self.dropout(att8)\n",
    "        pooled_output9 = torch.stack([ dd(att9)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        pooled_output10 = torch.stack([ dd(att10)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        pooled_output11 =torch.stack([ dd(att11)for dd in self.dropouts],dim=0).mean(dim=0)\n",
    "        \n",
    "        logits1 = self.classifier1(pooled_output1)\n",
    "        logits2 = self.classifier2(pooled_output2)\n",
    "        logits3 = self.classifier3(pooled_output3)\n",
    "        logits4 = self.classifier4(pooled_output4)\n",
    "        logits5 = self.classifier5(pooled_output5)\n",
    "        logits6 = self.classifier6(pooled_output6)\n",
    "        logits7 = self.classifier7(pooled_output7)\n",
    "        logits8 = self.classifier8(pooled_output8)\n",
    "        logits9 = self.classifier9(pooled_output9)\n",
    "        logits10 = self.classifier10(pooled_output10)\n",
    "        logits11 = self.classifier11(pooled_output11)\n",
    "     \n",
    "        \n",
    "        logits1=torch.cat([logits3,logits4,logits1,logits2[:,:2],logits5,logits2[:,2:5],logits6,logits2[:,5:]],dim=-1)\n",
    "\n",
    "        logits2=torch.cat([logits9,logits8[:,:-1],logits10,logits8[:,-1:],logits7,logits11],dim=-1)\n",
    "#         logits3=self.classifier_3(self.dropout(att1+att2+att3+att4+att5+att6+att7))\n",
    "        \n",
    "        predict1=self.predict(logits1)\n",
    "        predict2=self.predict(logits2)\n",
    "        \n",
    "#         predict3=self.predict(logits3)\n",
    "        outputs = (predict1,predict2)  # add hidden states and attention if they are here\n",
    "#         print('label:',labels)\n",
    "#         print('input_ids:',input_ids)\n",
    "#         print('attention_mas:',attention_mask)\n",
    "        if labels1 is not None:\n",
    "            loss_fct = nn.BCELoss()\n",
    "            loss_fct2=nn.MSELoss()\n",
    "#                 print(logits.view(-1, self.num_labels))\n",
    "#                 print(labels.view(-1, self.num_labels))\n",
    "            loss1 = loss_fct(predict1.view(-1, self.num_labels1), labels1.view(-1, self.num_labels1))\n",
    "            loss2 = loss_fct(predict2.view(-1, self.num_labels2), labels2.view(-1, self.num_labels2))\n",
    "#             labels3=torch.cat([labels1,labels2],dim=-1)\n",
    "#             loss3=loss_fct(predict3,labels3)\n",
    "#             predicts=[]\n",
    "#             for i in range(17):\n",
    "#                 predicts.append(loss_fct(predict1[:,i],labels1[:,i]).detach().cpu())\n",
    "#             for i in range(12):\n",
    "#                 predicts.append(loss_fct(predict2[:,i],labels2[:,i]).detach().cpu())\n",
    "            loss=loss1+loss2\n",
    "            outputs = (loss,loss) + outputs\n",
    "#         print(outputs)\n",
    "        return outputs  # (loss), predict1,predict2, (hidden_states), (attentions)\n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size,1)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :param src_len:\n",
    "            used for masking. NoneType or tensor in shape (B) indicating sequence length\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''   \n",
    "        att=self.attn(x)\n",
    "        att=F.tanh(att)\n",
    "        att=F.softmax(att,1)\n",
    "        att_x=att*x\n",
    "        return att_x.sum(1)\n",
    "import torch.utils.data as Data\n",
    "class CustomDataset(Data.Dataset):\n",
    "    def __init__(self, data, maxlen,tokenizer,with_labels=True, model_name='bert-base-chinese'):\n",
    "        self.data = data  # pandas dataframe\n",
    "\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = tokenizer#AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def get_label(self,x,num):\n",
    "        label=[0]*num\n",
    "       \n",
    "        x=x.strip().split(' ')\n",
    "\n",
    "        for l in x:              \n",
    "            if l and l!='nan':\n",
    "                label[int(l)]=1\n",
    "        return label\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        sent = str(self.data.loc[index, 'sentence'])\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent,\n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,       # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
    "#         print(encoded_pair['input_ids'])\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "#         token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            label1 = torch.Tensor(self.get_label(str(self.data.loc[index, 'label1']),17))\n",
    "            label2 = torch.Tensor(self.get_label(str(self.data.loc[index, 'label2']),12))\n",
    "            return token_ids, attn_masks,label1,label2\n",
    "        else:\n",
    "            return token_ids, attn_masks\n",
    "from sklearn.utils import shuffle as reset\n",
    "def train_test_split(data_df, test_size=0.2, shuffle=True, random_state=None):\n",
    "    if shuffle:\n",
    "        data_df = reset(data_df, random_state=random_state)\n",
    "\n",
    "    train = data_df[int(len(data_df)*test_size):].reset_index(drop = True)\n",
    "    test  = data_df[:int(len(data_df)*test_size)].reset_index(drop = True)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "from torch.nn.functional import cross_entropy,binary_cross_entropy\n",
    "\n",
    "\n",
    "def eval(model, optimizer, validation_dataloader,output_model = './train_class/model.pth'):\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            predict1,predict2 = model(batch[0], batch[1])\n",
    "            predict1,predict2 = predict1.detach().cpu(),predict2.detach().cpu()\n",
    "            label_ids1,label_ids2 = batch[2].cpu(),batch[3].cpu()\n",
    "            \n",
    "            tmp_eval_accuracy = binary_cross_entropy(predict1, label_ids1.float()).item()+binary_cross_entropy(predict2, label_ids2.float()).item()\n",
    "            \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation mlogloss: {}\".format(eval_accuracy / nb_eval_steps))\n",
    "    global best_score\n",
    "    if best_score > eval_accuracy / nb_eval_steps:\n",
    "        best_score = eval_accuracy / nb_eval_steps\n",
    "        save(model, optimizer,output_model)\n",
    "        return 0\n",
    "    return 1\n",
    "def save(model, optimizer,output_model):\n",
    "    # save\n",
    "    torch.save(model, output_model)\n",
    "    print('The best model has been saved')\n",
    "def flat_accuracy(preds, labels):\n",
    "#     print(preds,labels)\n",
    "    return -np.mean(labels*np.log(preds+1.e-7)+(1-labels)*np.log(preds+1.e-7))*10\n",
    "\n",
    "# 对抗训练\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "    def attack(self, epsilon=1, emb_name='word_emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "    def restore(self, emb_name='word_emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        \n",
    "from collections import defaultdict\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "\n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "\n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "class WarmupLinearSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then linear decay.\n",
    "        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n",
    "        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n",
    "        Linearly decreases the multiplicative variable from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1, self.warmup_steps))\n",
    "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        correct_bias=correct_bias)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                step_size = group['lr']\n",
    "                if group['correct_bias']:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state['step']\n",
    "                    bias_correction2 = 1.0 - beta2 ** state['step']\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                if group['weight_decay'] > 0.0:\n",
    "                    p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n",
    "        return loss\n",
    "def build_optimizer(model, train_steps, learning_rate):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, correct_bias=False, eps=1e-8)\n",
    "    optimizer = Lookahead(optimizer, 5, 1)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=train_steps * 0.1, t_total=train_steps)\n",
    "    return optimizer, scheduler\n",
    "def to_predict(model, dataloader,output_model, with_labels=False):\n",
    "    \n",
    "    # load model\n",
    "    checkpoint = torch.load(output_model, map_location='cuda')\n",
    "#     print(checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "\n",
    "    print('-----Testing-----')\n",
    "\n",
    "    pred_label =np.zeros((len(test),29))\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            predict1,predict2 = model(batch[0], batch[1])\n",
    "            predict1 = predict1.detach().cpu().numpy()\n",
    "            predict2 = predict2.detach().cpu().numpy()\n",
    "            predict=np.concatenate([predict1,predict2],axis=-1)\n",
    "            pred_label[i*batch_size:(i+1)*batch_size]=predict\n",
    "    return pred_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "satellite-symbol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T05:03:54.390766Z",
     "start_time": "2021-05-10T05:03:54.377070Z"
    }
   },
   "outputs": [],
   "source": [
    "class PGD():\n",
    "    def __init__(self, model, emb_name='word_emb', epsilon=1., alpha=0.3):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        self.model = model\n",
    "        self.emb_name = emb_name\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.emb_backup = {}\n",
    "        self.grad_backup = {}\n",
    "    def attack(self, is_first_attack=False):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                if is_first_attack:\n",
    "                    self.emb_backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = self.alpha * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = self.project(name, param.data, self.epsilon)\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                assert name in self.emb_backup\n",
    "                param.data = self.emb_backup[name]\n",
    "        self.emb_backup = {}\n",
    "    def project(self, param_name, param_data, epsilon):\n",
    "        r = param_data - self.emb_backup[param_name]\n",
    "        if torch.norm(r) > epsilon:\n",
    "            r = epsilon * r / torch.norm(r)\n",
    "        return self.emb_backup[param_name] + r\n",
    "    def backup_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                self.grad_backup[name] = param.grad.clone()\n",
    "    def restore_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                param.grad = self.grad_backup[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surface-stage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T05:03:54.739034Z",
     "start_time": "2021-05-10T05:03:54.736148Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "underlying-excitement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T05:03:55.440659Z",
     "start_time": "2021-05-10T05:03:55.328534Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../tcdata/train.csv',header=None)\n",
    "test=pd.read_csv('../tcdata/track1_round1_testB.csv',header=None)\n",
    "# test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "model_path='../model_weight/mpnet_new/'\n",
    "# model_vocab='../model_weight/mpnet_new/'\n",
    "output_model='../tmp/MPNet.pth'\n",
    "batch_size=32\n",
    "# 合并训练集与测试集 制作特征\n",
    "for i in range(1,3):\n",
    "    train[i]=train[i].apply(lambda x:x.replace('|','').strip())\n",
    "for i in range(1,2):\n",
    "    test[i]=test[i].apply(lambda x:x.replace('|','').strip())\n",
    "train.columns=['idx','sentence','label1','label2']\n",
    "test.columns=['idx','sentence']\n",
    "\n",
    "tokenizer=BertTokenizerFast('../tmp/vocab.txt')\n",
    "\n",
    "config=MPNetConfig.from_pretrained(model_path,num_labels=29,hidden_dropout_prob=0.3) # config.output_attentions=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "signal-coffee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T05:03:55.762725Z",
     "start_time": "2021-05-10T05:03:55.738111Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_df,val_df,test_oof):\n",
    "    \n",
    "        ###--------------------\n",
    "    early_stop=0\n",
    "    print(\"Reading training data...\")\n",
    "    train_set = CustomDataset(train_df, maxlen=128,tokenizer=tokenizer)\n",
    "    train_loader = Data.DataLoader(train_set, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "\n",
    "    print(\"Reading validation data...\")\n",
    "    val_set = CustomDataset(val_df, maxlen=128, tokenizer=tokenizer)\n",
    "    val_loader = Data.DataLoader(val_set, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "\n",
    "    test_set = CustomDataset(test, maxlen=128, tokenizer=tokenizer,with_labels=False)\n",
    "    test_loader = Data.DataLoader(test_set, batch_size=batch_size, num_workers=5, shuffle=False)\n",
    "    # 准备模型\n",
    "    model=NeZhaForSequenceClassification(config=config,model_name=model_path,num_labels1=17,num_labels2=12)\n",
    "#     model=torch.load('../tmp/MPNet.pth')\n",
    "    ### 训练\n",
    "    model.to(device)\n",
    "#     fgm = FGM(model)\n",
    "    pgd = PGD(model)\n",
    "    K = 3\n",
    "    \n",
    "    train_num = len(train_set)\n",
    "    train_steps = int(train_num * epochs / batch_size) + 1\n",
    "\n",
    "    optimizer, scheduler = build_optimizer(model, train_steps, learning_rate=6e-5)\n",
    "    print('-----Training-----')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        print('Epoch', epoch)\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            loss,s_loss, predict1,predict2 = model(batch[0], batch[1], batch[2],batch[3])\n",
    "            if i % 100 == 0:\n",
    "                print(i, s_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # 对抗训练\n",
    "#             fgm.attack()\n",
    "#             loss_adv,_, _,_  = model(batch[0], batch[1], batch[2],batch[3])\n",
    "#             loss_adv.backward()\n",
    "#             fgm.restore()\n",
    "#             pgd\n",
    "            pgd.backup_grad()\n",
    "    # 对抗训练\n",
    "            for t in range(K):\n",
    "                pgd.attack(is_first_attack=(t==0)) # 在embedding上添加对抗扰动, first attack时备份param.data\n",
    "                if t != K-1:\n",
    "                    model.zero_grad()\n",
    "                else:\n",
    "                    pgd.restore_grad()\n",
    "                loss_adv,_, _,_ = model(batch[0], batch[1], batch[2],batch[3])\n",
    "                loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "            pgd.restore() # 恢复embedding参数\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "   \n",
    "    #         if i % 20 == 0:\n",
    "    #             eval(model, optimizer, val_loader, output_model='./runs/nezha1.pth')\n",
    "        if epoch>-1:\n",
    "            early_stop+=eval(model, optimizer, val_loader, output_model=output_model)\n",
    "        if early_stop==2:\n",
    "            break\n",
    "\n",
    "    test_oof+=to_predict(model, test_loader,output_model, with_labels=False)/n_fold.n_splits\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return test_oof   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "statewide-lover",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T05:58:04.307808Z",
     "start_time": "2021-05-10T05:04:02.113859Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at ../model_weight/mpnet_new/ and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training-----\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lichangyv/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.415967345237732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-bb73e8888645>:445: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370117127/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      " 18%|█▊        | 100/547 [02:33<11:23,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.2684476673603058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [05:08<08:59,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.17145875096321106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [07:44<06:20,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.19518794119358063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [10:22<03:49,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.09244702756404877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [12:59<01:13,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.0952676311135292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [14:12<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.08192037935427661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 1\n",
      "0 0.05193082243204117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [02:38<11:46,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.07862133532762527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [05:15<08:58,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.07521286606788635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [07:53<06:26,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.056408703327178955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [10:30<03:52,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.05476207286119461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [13:07<01:13,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.029676467180252075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [14:21<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.055530765845404964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 2\n",
      "0 0.08679613471031189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [02:33<11:20,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.05158913880586624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [05:07<08:54,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.030150609090924263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [07:41<06:24,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.04072947055101395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [10:16<03:49,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.017647577449679375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 500/547 [12:51<01:13,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.03425046429038048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [14:03<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mlogloss: 0.04550901746165149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved\n",
      "Epoch 3\n",
      "0 0.02937319688498974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/547 [02:36<12:03,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.01390758901834488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/547 [05:12<09:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.020260360091924667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 300/547 [07:50<06:25,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.03997018188238144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 400/547 [10:28<03:52,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.014319618232548237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 408/547 [10:42<03:38,  1.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-db573dca250f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtest_oof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_oof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfold_s\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a0754794d52e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_df, val_df, test_oof)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mpgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mloss_adv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mloss_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 反向传播，并在正常的grad基础上，累加对抗训练的梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mpgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 恢复embedding参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_fold=KFold(8,shuffle=True,random_state=1080)\n",
    "test_oof=0\n",
    "epochs = 5\n",
    "fold_s=0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for trn_idx,val_idx in n_fold.split(train):\n",
    "    train_df=train.iloc[trn_idx].reset_index(drop=True)\n",
    "    val_df=train.iloc[val_idx].reset_index(drop=True)\n",
    "    best_score = float('inf')\n",
    "    test_oof=train_model(train_df,val_df,test_oof)\n",
    "    fold_s+=1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-candle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T12:57:53.916373Z",
     "start_time": "2021-05-03T12:44:33.918Z"
    }
   },
   "outputs": [],
   "source": [
    "         mpnet_0.07   0.28    MpPNET  -> 3 loss-> ADD LOSS -> 0.32     0.37      drop 0.1  drop 0.2~0.3\n",
    "epoch0 -> 0.0837  ->  0.076  ->0.069(0.075) ->0.074  ->  0.069 -> 0.084   ->0.08 ->0.075  0.0726   \n",
    "epoch1 -> 0.0555  -> 0.047 -<0.42    -> 0.0426 -> 0.0494-> 0.055\n",
    "                                      ->0.039  -> 0.041 ->  0.047\n",
    "                                                -> 0.038  ->0.041\n",
    "                                                           ->0.039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=0.3,13class mpnet_origin  0.0729     0.049    0.0417    0.038    0.036\n",
    "drop=0.3,13class mpnet_new     0.0831     0.056    0.0456    0.049    0.040\n",
    "drop=0.3,21class mpnet_origin  0.0737     0.0528    0.0418   0.038    0.037 （过拟合之嫌疑）\n",
    "drop=0.3,7class mpnet_origin   0.721       0.049    0.04519  0.039    0.0382 （过拟合相对较少）\n",
    "add_mse ------------------- 0.07299     0.0574    0.0423    0.0381    0.0370\n",
    " fgm                           0.0719     0.05434    0.04039   0.03821   0.03789\n",
    "11class                        0.0711     0.0494     0.0405     0.371   0.0356\n",
    "alpha1                            0.07038\n",
    "mpnet_new                      0.766        0.0538    0.4373\n",
    "\n",
    "                               0.779        0.536     0.4530    0.039   0.037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "test=pd.read_csv('../tcdata/testA.csv',header=None)\n",
    "sub['report_ID']=test[0]\n",
    "sub['Prediction']=[ '|'+' '.join(['%.12f'%j for j in i]) for i in test_oof ]\n",
    "sub.to_csv('../result.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
